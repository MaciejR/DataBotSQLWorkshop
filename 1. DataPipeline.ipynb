{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ćwiczenie: Budowa pipeline dla danych ustrukturyzowanych i nieustrukturyzowanych\n",
    "\n",
    "**Czas trwania:** 60 minut\n",
    "\n",
    "**Cel ćwiczenia:** Stworzymy kompleksowy pipeline danych, który będzie pobierał dane z różnych źródeł, zarówno ustrukturyzowanych jak i nieustrukturyzowanych, i przygotowywał je do dalszego przetwarzania przez DataBota."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Wprowadzenie do pipeline'u danych dla RAG\n",
    "\n",
    "Retrieval Augmented Generation (RAG) to podejście, które łączy możliwości generatywne modeli językowych z dostępem do zewnętrznych źródeł danych. Aby zbudować efektywny DataBot, musimy stworzyć pipeline danych, który pozwoli na:\n",
    "\n",
    "1. Pobieranie danych z różnych źródeł organizacyjnych\n",
    "2. Przetwarzanie tych danych do formatu zrozumiałego dla modelu LLM\n",
    "3. Przechowywanie i indeksowanie danych w sposób umożliwiający szybkie wyszukiwanie\n",
    "\n",
    "W tym ćwiczeniu skupimy się na trzech typach danych:\n",
    "- **Dane nieustrukturyzowane**: dokumenty tekstowe (.docx, .pdf) przechowywane w SharePoint/OneDrive\n",
    "- **Dane ustrukturyzowane**: dane tabelaryczne przechowywane w bazie danych SQL\n",
    "- **Dane półstrukturalne**: pliki CSV i JSON przechowywane w Azure Data Lake Storage\n",
    "\n",
    "Referencyjną architekturę RAG dla Microsoft Azure można znaleźć pod adresem: [Microsoft RAG Solution Design and Evaluation Guide](https://learn.microsoft.com/en-us/azure/architecture/ai-ml/guide/rag/rag-solution-design-and-evaluation-guide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyodbc\n",
    "!pip install azure-storage-file-datalake\n",
    "!pip install fitz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Połączenie z bazą danych SQL\n",
    "\n",
    "Pierwszym krokiem jest uzyskanie dostępu do danych strukturalnych przechowywanych w bazie danych SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Połączenie z bazą danych SQL\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "\n",
    "# Parametry połączenia - powinny być przechowywane w bezpiecznym miejscu (np. Key Vault)\n",
    "server = 'your-server.database.windows.net'\n",
    "database = 'your-database'\n",
    "username = 'your-username'\n",
    "password = 'your-password'\n",
    "driver = '{ODBC Driver 17 for SQL Server}'\n",
    "\n",
    "# String połączenia\n",
    "conn_str = f'DRIVER={driver};SERVER={server};DATABASE={database};UID={username};PWD={password}'\n",
    "\n",
    "try:\n",
    "    # Nawiązanie połączenia\n",
    "    conn = pyodbc.connect(conn_str)\n",
    "    print(\"Połączono z bazą danych SQL\")\n",
    "    \n",
    "    # Przykładowe zapytanie\n",
    "    query = \"SELECT TOP 10 * FROM Customers\"  # Dostosuj do swojej struktury bazy danych\n",
    "    \n",
    "    # Wykonanie zapytania i pobranie wyników jako DataFrame\n",
    "    df = pd.read_sql(query, conn)\n",
    "    print(f\"Pobrano {len(df)} wierszy\")\n",
    "    display(df.head())\n",
    "    \n",
    "    # Zamknięcie połączenia\n",
    "    conn.close()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Błąd podczas łączenia z bazą danych: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Praca z danymi nieustrukturyzowanymi w Azure Data Lake\n",
    "\n",
    "Teraz zajmiemy się pobieraniem i przetwarzaniem danych przechowywanych w Azure Data Lake Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Praca z plikami w Azure Data Lake Storage\n",
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "import pandas as pd\n",
    "import json\n",
    "import io\n",
    "import fitz\n",
    "\n",
    "# Parametry połączenia\n",
    "storage_account_name = \"\"\n",
    "storage_account_key = \"\"  # Powinien być przechowywany w bezpiecznym miejscu\n",
    "file_system_name = \"\"  # Nazwa kontenera ADLS\n",
    "\n",
    "# Inicjalizacja klienta usługi\n",
    "service_client = DataLakeServiceClient(\n",
    "    account_url=f\"https://{storage_account_name}.dfs.core.windows.net\",\n",
    "    credential=storage_account_key\n",
    ")\n",
    "\n",
    "# Pobierz referencję do systemu plików\n",
    "file_system_client = service_client.get_file_system_client(file_system_name)\n",
    "\n",
    "try:\n",
    "    pdf_path = \"\"\n",
    "    file_client = file_system_client.get_file_client(pdf_path)\n",
    "    \n",
    "\n",
    "    with fitz.open(\"plik.pdf\") as doc:\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "\n",
    "    print(text[:1000])  # Wyświetl pierwsze 1000 znaków\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Wystąpił błąd: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Walidacja połączenia i testowy odczyt danych\n",
    "\n",
    "Zanim przejdziemy dalej, powinniśmy przetestować wszystkie nasze połączenia, aby upewnić się, że działają poprawnie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walidacja połączeń - szablon\n",
    "# Zastąp własnymi wartościami\n",
    "\n",
    "def validate_connections():\n",
    "    results = {}\n",
    "    \n",
    "   \n",
    "    # Test 1: Połączenie z bazą danych SQL\n",
    "    try:\n",
    "        # Tutaj kod do testu połączenia z SQL\n",
    "        # ...\n",
    "        results[\"sql\"] = \"Sukces\"\n",
    "    except Exception as e:\n",
    "        results[\"sql\"] = f\"Błąd: {str(e)}\"\n",
    "    \n",
    "    # Test 2: Połączenie z Azure Data Lake\n",
    "    try:\n",
    "        # Tutaj kod do testu połączenia z ADLS\n",
    "        # ...\n",
    "        results[\"adls\"] = \"Sukces\"\n",
    "    except Exception as e:\n",
    "        results[\"adls\"] = f\"Błąd: {str(e)}\"\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Uruchom walidację\n",
    "# validation_results = validate_connections()\n",
    "# for source, status in validation_results.items():\n",
    "#     print(f\"Połączenie z {source}: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Integracja danych w jednym pipeline\n",
    "\n",
    "Ostatnim krokiem jest integracja wszystkich źródeł danych w jednym spójnym pipeline'u, który będzie mógł być wykorzystany przez DataBota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integracja wszystkich źródeł danych w jednym pipeline\n",
    "\n",
    "class DataPipeline:\n",
    "    def __init__(self):\n",
    "        # Inicjalizacja połączeń\n",
    "        self.init_connections()\n",
    "    \n",
    "    def init_connections(self):\n",
    "        # Inicjalizacja połączeń do różnych źródeł danych\n",
    "        # Kod inicjalizacji SQL, ADLS...\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def get_data_from_sql(self, query):\n",
    "        # Pobieranie danych z SQL\n",
    "        # ...\n",
    "        sql_data = None\n",
    "        return sql_data\n",
    "    \n",
    "    def get_data_from_adls(self, file_path):\n",
    "        # Pobieranie danych z ADLS\n",
    "        # ...\n",
    "        adls_data = None\n",
    "        return adls_data\n",
    "    \n",
    "    def process_documents(self, documents):\n",
    "        # Przetwarzanie dokumentów, np. ekstrakcja tekstu\n",
    "        # ...\n",
    "        processed_docs = []\n",
    "        return processed_docs\n",
    "    \n",
    "    def process_structured_data(self, data):\n",
    "        # Przetwarzanie danych strukturalnych\n",
    "        # ...\n",
    "        processed_data = None\n",
    "        return processed_data\n",
    "    \n",
    "    def run_pipeline(self, params):\n",
    "        # Uruchomienie pełnego pipeline'u\n",
    "        results = {}\n",
    "        \n",
    "        \n",
    "        # Pobierz i przetwórz dane SQL\n",
    "        if params.get(\"sql_enabled\", True):\n",
    "            sql_data = self.get_data_from_sql(params.get(\"sql_query\"))\n",
    "            results[\"sql_data\"] = self.process_structured_data(sql_data)\n",
    "        \n",
    "        # Pobierz i przetwórz dane z ADLS\n",
    "        if params.get(\"adls_enabled\", True):\n",
    "            adls_data = self.get_data_from_adls(params.get(\"adls_path\"))\n",
    "            results[\"adls_data\"] = self.process_structured_data(adls_data)\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Przykład użycia\n",
    "pipeline = DataPipeline()\n",
    "pipeline_params = {\n",
    "    \"documents_enabled\": True,\n",
    "    \"folder_path\": \"Documents/DataBotWorkshop\",\n",
    "    \"sql_enabled\": True,\n",
    "    \"sql_query\": \"SELECT * FROM Customers WHERE Region = 'Europe'\",\n",
    "    \"adls_enabled\": True,\n",
    "    \"adls_path\": \"data/sales_data.csv\"\n",
    "}\n",
    "\n",
    "# Uruchomienie pipeline'u\n",
    "# results = pipeline.run_pipeline(pipeline_params)\n",
    "# print(f\"Pipeline zakończony, zebrano dane z {len(results)} źródeł\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Zadania do wykonania\n",
    "\n",
    "1. Uzupełnij brakujące fragmenty kodu w klasie `DataPipeline`\n",
    "2. Zaimplementuj metodę `process_documents`, która będzie ekstrahować tekst z różnych typów dokumentów\n",
    "3. Zaimplementuj metodę `process_structured_data`, która będzie przetwarzać dane analityczne\n",
    "4. Uruchom pipeline na przykładowych danych i sprawdź, czy działa poprawnie\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
