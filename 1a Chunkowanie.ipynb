{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54076a8a",
   "metadata": {},
   "source": [
    "## Wprowadzenie\n",
    "\n",
    "W tym notatniku rozbudujemy podstawowy pipeline danych z ƒÜwiczenia 1 o zaawansowane techniki przetwarzania tekstu zgodnie z architekturƒÖ RAG:\n",
    "\n",
    "1. Preprocessing danych\n",
    "2. Techniki chunkowania\n",
    "3. Generowanie embeding√≥w\n",
    "4. Indeksowanie i przygotowanie do wyszukiwania\n",
    "\n",
    "Celem jest przygotowanie danych do efektywnego wykorzystania w modelu RAG (Retrieval-Augmented Generation).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed97169a",
   "metadata": {},
   "source": [
    "## 1. Instalacja niezbƒôdnych bibliotek\n",
    "\n",
    "Do przetwarzania tekstu wykorzystamy popularne biblioteki jak LangChain, kt√≥re u≈ÇatwiajƒÖ pracƒô z danymi nieustrukturyzowanymi.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cef5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instalacja niezbƒôdnych bibliotek\n",
    "%pip install langchain unstructured PyPDF2 sentence-transformers langchain-community markdown\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ddd19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install markdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f166e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import bibliotek\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, TextLoader, DirectoryLoader, UnstructuredMarkdownLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import re\n",
    "from pyspark.ml.feature import MinHashLSH\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import ArrayType, FloatType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e19d98a",
   "metadata": {},
   "source": [
    "### üîß ƒÜwiczenie 1: Dodaj w≈Çasny dokument Markdown\n",
    "Za≈Çadowanie wcze≈õniej przygotowanych danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621fbaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Odczyt danych ustrukturyzowanych z poprzedniego ƒáwiczenia\n",
    "# Je≈õli w ƒÜwiczeniu 1 zapisali≈õmy dane do tabel Delta, mo≈ºemy je odczytaƒá:\n",
    "try:\n",
    "    df_structured = spark.table(\"customer_data\")\n",
    "    print(f\"Odczytano dane ustrukturyzowane, liczba wierszy: {df_structured.count()}\")\n",
    "except:\n",
    "    print(\"Brak tabeli customer_data - utw√≥rz przyk≈Çadowe dane\")\n",
    "    \n",
    "    # Przyk≈Çadowe dane je≈õli tabela nie istnieje\n",
    "    data = [(\"1\", \"Przewodnik u≈ºytkownika\", \"To jest przewodnik u≈ºytkownika opisujƒÖcy funkcje produktu...\"),\n",
    "            (\"2\", \"FAQ\", \"Najczƒô≈õciej zadawane pytania dotyczƒÖce instalacji i konfiguracji...\"),\n",
    "            (\"3\", \"Instrukcja techniczna\", \"Specyfikacja techniczna produktu zawierajƒÖca szczeg√≥≈Çowe parametry...\")]\n",
    "    \n",
    "    df_structured = spark.createDataFrame(data, [\"id\", \"title\", \"content\"])\n",
    "    df_structured.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"customer_data\")\n",
    "    print(f\"Utworzono przyk≈Çadowe dane, liczba wierszy: {df_structured.count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0717a0",
   "metadata": {},
   "source": [
    "## 2. Odczyt danych nieustrukturyzowanych z poprzedniego ƒáwiczenia\n",
    "\n",
    "Za≈Ç√≥≈ºmy, ≈ºe w poprzednim ƒáwiczeniu po≈ÇƒÖczyli≈õmy siƒô z r√≥≈ºnymi ≈∫r√≥d≈Çami danych i zapisali≈õmy je w formacie Delta Lake. Teraz odczytamy te dane i przygotujemy je do dalszego przetwarzania.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e055142e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ≈öcie≈ºka do Volume (uwaga: /Volumes/<catalog>/<schema>/<volume_name>/)\n",
    "volume_path = \"/Volumes/sqlday_edbworkshop/default/workshop_volume/docs/\"\n",
    "\n",
    "import os\n",
    "\n",
    "# Upewnij siƒô, ≈ºe folder docelowy istnieje (mo≈ºesz to pominƒÖƒá ‚Äì os.makedirs nie musi byƒá konieczne dla Volume)\n",
    "os.makedirs(volume_path, exist_ok=True)\n",
    "\n",
    "sample_texts = [\n",
    "    \"# Dokumentacja produktu\\n\\nNasz produkt oferuje zaawansowane funkcje analityczne...\\n\\n## Instalacja\\n\\nAby zainstalowaƒá produkt, wykonaj nastƒôpujƒÖce kroki...\\n\\n## Konfiguracja\\n\\nKonfiguracja produktu wymaga...\",\n",
    "    \"# Przewodnik u≈ºytkownika\\n\\nW tym przewodniku opisujemy krok po kroku jak korzystaƒá z...\\n\\n## Funkcje podstawowe\\n\\nPodstawowe funkcje obejmujƒÖ...\\n\\n## Funkcje zaawansowane\\n\\nZaawansowane funkcje pozwalajƒÖ na...\",\n",
    "    \"# FAQ\\n\\n## Jak zresetowaƒá has≈Ço?\\n\\nAby zresetowaƒá has≈Ço, przejd≈∫ do ustawie≈Ñ...\\n\\n## Jak utworzyƒá nowy projekt?\\n\\nAby utworzyƒá nowy projekt, kliknij przycisk 'Nowy projekt'...\"\n",
    "]\n",
    "\n",
    "# Zapis plik√≥w do Volume\n",
    "for i, text in enumerate(sample_texts):\n",
    "    with open(f\"{volume_path}dokument_{i+1}.md\", \"w\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "print(f\"‚úÖ Utworzono przyk≈Çadowe pliki w: {volume_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53ea512",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.document_loaders import DirectoryLoader, UnstructuredMarkdownLoader\n",
    "\n",
    "try:\n",
    "    loader = DirectoryLoader(\n",
    "        path=volume_path,\n",
    "        glob=\"**/*.md\",\n",
    "        loader_cls=UnstructuredMarkdownLoader\n",
    "    )\n",
    "    documents = loader.load()\n",
    "    print(f\"‚úÖ Za≈Çadowano {len(documents)} dokument√≥w\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå B≈ÇƒÖd przy ≈Çadowaniu dokument√≥w: {e}\")\n",
    "    documents = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5ef490",
   "metadata": {},
   "source": [
    "## 3. Preprocessing danych\n",
    "\n",
    "Przed chunkingiem nale≈ºy przeprowadziƒá czyszczenie i wzbogacanie danych:\n",
    "- Czyszczenie tekstu\n",
    "- Dodanie metadanych\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986881d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3.1 Czyszczenie tekstu\n",
    "def clean_text(text):\n",
    "    \"\"\"Funkcja czyszczƒÖca tekst z niepotrzebnych znak√≥w i formatowania\"\"\"\n",
    "    # Usuwanie nadmiarowych bia≈Çych znak√≥w\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Usuwanie znak√≥w specjalnych, kt√≥re mogƒÖ zak≈Ç√≥caƒá analizƒô\n",
    "    text = re.sub(r'[^\\w\\s.,;:!?()-]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Czyszczenie tekst√≥w w dokumentach\n",
    "cleaned_documents = []\n",
    "for doc in documents:\n",
    "    cleaned_text = clean_text(doc.page_content)\n",
    "    doc.page_content = cleaned_text\n",
    "    cleaned_documents.append(doc)\n",
    "\n",
    "# 3.2 Dodanie metadanych\n",
    "for i, doc in enumerate(cleaned_documents):\n",
    "    # Dodanie dodatkowych metadanych\n",
    "    doc.metadata[\"doc_id\"] = f\"doc_{i}\"\n",
    "    doc.metadata[\"doc_type\"] = \"markdown\"  # Lub inny typ, w zale≈ºno≈õci od rzeczywistych danych\n",
    "    doc.metadata[\"importance\"] = \"high\" if \"FAQ\" in doc.page_content else \"medium\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a0559f",
   "metadata": {},
   "source": [
    "## 4. Chunkowanie danych\n",
    "\n",
    "Implementacja r√≥≈ºnych strategii chunkowania tekstu:\n",
    "1. Fixed-size chunking - sta≈Çej wielko≈õci\n",
    "2. Paragraph-based chunking - na podstawie paragraf√≥w\n",
    "3. Format-specific chunking - bazujƒÖce na formatowaniu\n",
    "4. Semantic chunking - semantyczne (om√≥wimy koncepcjƒô)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce67a43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4.1 Fixed-size chunking\n",
    "def fixed_size_chunking(documents, chunk_size=500, chunk_overlap=50):\n",
    "    \"\"\"Dzieli dokumenty na chunki o sta≈Çej wielko≈õci\"\"\"\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\\n\",\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    \n",
    "    chunks = []\n",
    "    for doc in documents:\n",
    "        doc_chunks = text_splitter.create_documents(\n",
    "            [doc.page_content], \n",
    "            metadatas=[doc.metadata]\n",
    "        )\n",
    "        chunks.extend(doc_chunks)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# 4.2 Paragraph-based chunking\n",
    "def paragraph_based_chunking(documents, chunk_size=1000, chunk_overlap=100):\n",
    "    \"\"\"Dzieli dokumenty na chunki bazujƒÖc na paragrafach\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    \n",
    "    chunks = []\n",
    "    for doc in documents:\n",
    "        doc_chunks = text_splitter.create_documents(\n",
    "            [doc.page_content], \n",
    "            metadatas=[doc.metadata]\n",
    "        )\n",
    "        chunks.extend(doc_chunks)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# 4.3 Format-specific chunking (dla Markdown)\n",
    "def markdown_header_chunking(documents):\n",
    "    \"\"\"Dzieli dokumenty Markdown na podstawie nag≈Ç√≥wk√≥w\"\"\"\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=[\n",
    "            (\"#\", \"header_1\"),\n",
    "            (\"##\", \"header_2\"),\n",
    "            (\"###\", \"header_3\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    chunks = []\n",
    "    for doc in documents:\n",
    "        # Pobierz oryginalne metadane\n",
    "        original_metadata = doc.metadata\n",
    "        \n",
    "        # Podziel tekst wed≈Çug nag≈Ç√≥wk√≥w Markdown\n",
    "        md_header_splits = markdown_splitter.split_text(doc.page_content)\n",
    "        \n",
    "        # Dodaj oryginalne metadane do ka≈ºdego chunka\n",
    "        for chunk in md_header_splits:\n",
    "            # Po≈ÇƒÖcz metadane z podzia≈Çu z oryginalnymi metadanymi\n",
    "            combined_metadata = {**chunk.metadata, **original_metadata}\n",
    "            chunk.metadata = combined_metadata\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# 4.4 Semantic chunking (koncepcja)\n",
    "def semantic_chunking_concept():\n",
    "    \"\"\"\n",
    "    Wyja≈õnienie koncepcji semantic chunking.\n",
    "    \n",
    "    W rzeczywistej implementacji u≈ºywaliby≈õmy modelu embedowania do analizy semantycznej i \n",
    "    dzielenia tekstu na podstawie zmian w tematyce.\n",
    "    \n",
    "    Mo≈ºliwe implementacje:\n",
    "    1. U≈ºycie algorytm√≥w segmentacji tematu\n",
    "    2. U≈ºycie klastrowania embedding√≥w zda≈Ñ\n",
    "    3. Wykrywanie zmian w wektorach embedding√≥w dla sƒÖsiednich fragment√≥w tekstu\n",
    "    \"\"\"\n",
    "    print(\"\"\"\n",
    "    Semantic Chunking:\n",
    "    \n",
    "    1. Obliczanie embedding√≥w dla ka≈ºdego zdania lub paragrafu\n",
    "    2. Wykrywanie naturalnych granic tematycznych przez analizƒô odleg≈Ço≈õci wektor√≥w\n",
    "    3. Grupowanie zda≈Ñ o podobnej semantyce\n",
    "    4. Dzielenie na chunki w miejscach, gdzie nastƒôpuje wiƒôksza zmiana semantyczna\n",
    "    \n",
    "    W praktyce wymaga to:\n",
    "    - Modelu embedding√≥w (np. sentence-transformers)\n",
    "    - Algorytmu segmentacji (np. TextTiling)\n",
    "    - Technik klastrowania (np. DBSCAN, k-means)\n",
    "    \"\"\")\n",
    "    \n",
    "    return None  # W rzeczywistej implementacji zwracaliby≈õmy chunki\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c70689",
   "metadata": {},
   "source": [
    "### üîß ƒÜwiczenie 1: Przetestuj r√≥≈ºne formy chunkowania\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fafe02",
   "metadata": {},
   "source": [
    "## 5. Generowanie embedding√≥w\n",
    "\n",
    "Konwersja chunk√≥w tekstu na wektory embedding√≥w, kt√≥re bƒôdƒÖ u≈ºywane do wyszukiwania podobie≈Ñstwa semantycznego.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c77b159",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Inicjalizacja modelu embedding√≥w\n",
    "# W produkcyjnym ≈õrodowisku mo≈ºna u≈ºyƒá OpenAI, Azure OpenAI lub innych modeli\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"  # Ma≈Çy model do cel√≥w demonstracyjnych\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "# Generowanie embedding√≥w dla chunk√≥w tekstu\n",
    "if len(selected_chunks) > 0:\n",
    "    # Generowanie embedding√≥w\n",
    "    chunk_texts = [chunk.page_content for chunk in selected_chunks]\n",
    "    embeddings = embedding_model.embed_documents(chunk_texts)\n",
    "    \n",
    "    print(f\"Wygenerowano {len(embeddings)} embedding√≥w.\")\n",
    "    print(f\"Wymiar embeddingu: {len(embeddings[0])}\")\n",
    "    \n",
    "    # Konwersja na format DataFrame do dalszego przetwarzania\n",
    "    embedding_rows = []\n",
    "    for i, (chunk, emb) in enumerate(zip(selected_chunks, embeddings)):\n",
    "        # Tworzymy wiersz z chunkiem tekstu, metadanymi i embeddingiem\n",
    "        metadata_str = str(chunk.metadata)  # Konwersja s≈Çownika metadanych na string\n",
    "        embedding_rows.append((i, chunk.page_content, metadata_str, emb))\n",
    "    \n",
    "    # Schemat DataFrame\n",
    "    from pyspark.sql.types import StructType, StructField, IntegerType, StringType, ArrayType, FloatType\n",
    "    \n",
    "    embedding_schema = StructType([\n",
    "        StructField(\"id\", IntegerType(), False),\n",
    "        StructField(\"content\", StringType(), True),\n",
    "        StructField(\"metadata\", StringType(), True),\n",
    "        StructField(\"embedding\", ArrayType(FloatType()), True)\n",
    "    ])\n",
    "    \n",
    "    # Tworzenie DataFrame\n",
    "    embedding_df = spark.createDataFrame(embedding_rows, schema=embedding_schema)\n",
    "    \n",
    "    # Zapisanie do tabeli Delta dla dalszego u≈ºycia\n",
    "    embedding_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"document_embeddings\")\n",
    "    \n",
    "    print(\"Zapisano embeddingi do tabeli 'document_embeddings'\")\n",
    "else:\n",
    "    print(\"Brak chunk√≥w do generowania embedding√≥w\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b505ecfb",
   "metadata": {},
   "source": [
    "## 6. Indeksowanie i przechowywanie\n",
    "\n",
    "Przygotowanie indeksu wektorowego do efektywnego wyszukiwania podobnych dokument√≥w.\n",
    "\n",
    "W Databricks mo≈ºemy wykorzystaƒá Mosaic AI Vector Search do tego celu.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b40818",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Konfiguracja Mosaic AI Vector Search w Databricks\n",
    "# W rzeczywistym ≈õrodowisku nale≈ºy u≈ºyƒá odpowiednich parametr√≥w dla klastra\n",
    "try:\n",
    "    # Sprawdzenie czy tabela z embeddingami istnieje\n",
    "    embedding_count = spark.table(\"document_embeddings\").count()\n",
    "    \n",
    "    if embedding_count > 0:\n",
    "        # Kreowanie Vector Search Index\n",
    "        # Uwaga: Poni≈ºszy kod jest koncepcyjny i mo≈ºe wymagaƒá dostosowania\n",
    "        # do rzeczywistej wersji Databricks i dostƒôpnych funkcji\n",
    "        \n",
    "        print(\"\"\"\n",
    "        Krok koncepcyjny - tworzenie indeksu wektorowego:\n",
    "        \n",
    "        W Databricks Mosaic Vector Search utworzyliby≈õmy indeks:\n",
    "        \n",
    "        1. W interfejsie Databricks:\n",
    "           - Przejd≈∫ do zak≈Çadki \"Catalog\"\n",
    "           - Wybierz \"Vector Search\" \n",
    "           - Utw√≥rz nowy indeks wskazujƒÖc tabelƒô \"document_embeddings\"\n",
    "           - Wybierz kolumnƒô \"embedding\" jako ≈∫r√≥d≈Ço wektor√≥w\n",
    "           - Skonfiguruj parametry indeksu (np. wymiar, metrykƒô)\n",
    "        \n",
    "        2. Alternatywnie za pomocƒÖ kodu:\n",
    "           ```\n",
    "           from databricks.vector_search.client import VectorSearchClient\n",
    "           \n",
    "           vsc = VectorSearchClient()\n",
    "           \n",
    "           vsc.create_index(\n",
    "               index_name=\"document_search_index\",\n",
    "               primary_key=\"id\",\n",
    "               embedding_source={\n",
    "                   \"table_name\": \"document_embeddings\",\n",
    "                   \"embedding_column\": \"embedding\",\n",
    "                   \"dim\": len(embeddings[0])\n",
    "               },\n",
    "               fields=[{\"name\": \"content\", \"type\": \"STRING\"}, \n",
    "                      {\"name\": \"metadata\", \"type\": \"STRING\"}]\n",
    "           )\n",
    "           ```\n",
    "        \n",
    "        3. Po utworzeniu indeksu mo≈ºemy wykonywaƒá wyszukiwania:\n",
    "           ```\n",
    "           query_embedding = embedding_model.embed_query(\"Jak zresetowaƒá has≈Ço?\")\n",
    "           \n",
    "           results = vsc.query(\n",
    "               index_name=\"document_search_index\",\n",
    "               query_vector=query_embedding,\n",
    "               num_results=5\n",
    "           )\n",
    "           ```\n",
    "        \"\"\")\n",
    "    else:\n",
    "        print(\"Brak danych w tabeli 'document_embeddings'\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"B≈ÇƒÖd przy dostƒôpie do danych: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2e4fd0",
   "metadata": {},
   "source": [
    "## 7. Testowanie wyszukiwania (symulacja)\n",
    "\n",
    "Symulacja wyszukiwania w indeksie wektorowym przy u≈ºyciu embedding√≥w.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f2f6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Symulacja wyszukiwania (bez faktycznego indeksu Vector Search)\n",
    "if 'embedding_model' in locals() and len(selected_chunks) > 0:\n",
    "    # Zapytanie testowe\n",
    "    query = \"Jak zresetowaƒá has≈Ço u≈ºytkownika?\"\n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "    \n",
    "    print(f\"Zapytanie: '{query}'\")\n",
    "    print(f\"Wygenerowano embedding zapytania o wymiarze {len(query_embedding)}\")\n",
    "    \n",
    "    # Funkcja do obliczania podobie≈Ñstwa cosinusowego\n",
    "    def cosine_similarity(vec1, vec2):\n",
    "        dot_product = np.dot(vec1, vec2)\n",
    "        norm_vec1 = np.linalg.norm(vec1)\n",
    "        norm_vec2 = np.linalg.norm(vec2)\n",
    "        return dot_product / (norm_vec1 * norm_vec2)\n",
    "    \n",
    "    # Symulacja wyszukiwania przez obliczenie podobie≈Ñstwa z ka≈ºdym chunkiem\n",
    "    similarities = []\n",
    "    for i, (chunk, emb) in enumerate(zip(selected_chunks, embeddings)):\n",
    "        sim_score = cosine_similarity(query_embedding, emb)\n",
    "        similarities.append((i, chunk, sim_score))\n",
    "    \n",
    "    # Sortowanie wynik√≥w wed≈Çug podobie≈Ñstwa (malejƒÖco)\n",
    "    similarities.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    # Wy≈õwietlenie najlepszych dopasowa≈Ñ\n",
    "    print(\"\\nNajlepsze dopasowania:\")\n",
    "    for i, (chunk_id, chunk, score) in enumerate(similarities[:3]):\n",
    "        print(f\"\\nWynik {i+1} (Podobie≈Ñstwo: {score:.4f}):\")\n",
    "        print(f\"Tre≈õƒá: {chunk.page_content[:150]}...\")\n",
    "        print(f\"Metadane: {chunk.metadata}\")\n",
    "else:\n",
    "    print(\"Brak danych do symulacji wyszukiwania\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074a0959",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed9bc7c",
   "metadata": {},
   "source": [
    "## 8. Podsumowanie i dalsze kroki\n",
    "\n",
    "W tym notatniku zaimplementowali≈õmy:\n",
    "\n",
    "1. Preprocessing danych tekstowych\n",
    "2. R√≥≈ºne strategie chunkowania\n",
    "3. Generowanie embedding√≥w\n",
    "4. Koncepcjƒô indeksowania wektorowego\n",
    "5. Symulacjƒô wyszukiwania semantycznego\n",
    "\n",
    "W kolejnym ƒáwiczeniu wykorzystamy te komponenty do budowy pe≈Çnego systemu RAG (Retrieval-Augmented Generation) integrujƒÖcego model jƒôzykowy.\n",
    "\n",
    "**Dalsze kroki:**\n",
    "- Integracja z LLM (np. Azure OpenAI)\n",
    "- Budowa interfejsu zapyta≈Ñ\n",
    "- Ewaluacja i optymalizacja wynik√≥w\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
