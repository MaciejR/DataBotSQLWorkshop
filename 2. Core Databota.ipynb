{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ƒÜwiczenie: Budowa core DataBota\n",
    "\n",
    "**Czas trwania:** 90 minut\n",
    "\n",
    "**Cel ƒáwiczenia:** W tym ƒáwiczeniu stworzymy szkielet DataBota, kt√≥ry bƒôdzie w stanie przetwarzaƒá zapytania u≈ºytkownika, pobieraƒá odpowiednie dane i generowaƒá odpowiedzi przy u≈ºyciu modelu LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Wprowadzenie do przetwarzania zapyta≈Ñ u≈ºytkownika\n",
    "\n",
    "Nasza architektura DataBota bƒôdzie oparta na nastƒôpujƒÖcym pipeline przetwarzania zapyta≈Ñ:\n",
    "\n",
    "1. U≈ºytkownik wprowadza zapytanie w jƒôzyku naturalnym\n",
    "2. Zapytanie jest konwertowane na wektor embeddings\n",
    "3. System wykonuje wyszukiwanie semantyczne w bazie danych wektorowej\n",
    "4. Znalezione informacje sƒÖ przekazywane jako kontekst do modelu LLM\n",
    "5. Model LLM generuje odpowied≈∫ na podstawie zapytania i kontekstu\n",
    "\n",
    "Ta architektura, znana jako Retrieval Augmented Generation (RAG), pozwala na wykorzystanie zewnƒôtrznych ≈∫r√≥de≈Ç danych w procesie generowania odpowiedzi, co znacznie zwiƒôksza dok≈Çadno≈õƒá i aktualno≈õƒá informacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalacja wymaganych bibliotek\n",
    "%pip install openai langchain tiktoken pymupdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Import bibliotek\n",
    "import openai\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from langchain import LLMChain\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Sprawdzenie, czy wszystkie potrzebne biblioteki zosta≈Çy poprawnie zaimportowane\n",
    "print(\"≈örodowisko zosta≈Ço zainicjalizowane pomy≈õlnie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Konfiguracja dostƒôpu do Azure OpenAI\n",
    "\n",
    "Aby korzystaƒá z modeli OpenAI, musimy skonfigurowaƒá dostƒôp do Azure OpenAI Service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "endpoint = \"https://aissqldayedbworkshop001.openai.azure.com/\"\n",
    "model_name = \"o3-mini\"\n",
    "deployment = \"o3-mini\"\n",
    "\n",
    "subscription_key=\"4mE2kj9PLeZ0NjqMiFzgxtStKtJIDRnZ4dzNIsipygEDdbYHmlCXJQQJ99BEAC5RqLJXJ3w3AAAAACOG85N1\"\n",
    "api_version = \"2024-12-01-preview\"\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_version=api_version,\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=subscription_key,\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"I am going to Paris, what should I see?\",\n",
    "        }\n",
    "    ],\n",
    "    max_completion_tokens=100000,\n",
    "    model=deployment\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konfiguracja dostƒôpu do Azure OpenAI\n",
    "# W ≈õrodowisku produkcyjnym te dane powinny byƒá przechowywane w Azure Key Vault lub Databricks Secrets\n",
    "\n",
    "# Dla Azure OpenAI Service\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2023-05-15\"  # Dostosuj do aktualnej wersji API\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://your-azure-openai-resource.openai.azure.com/\"  # ZastƒÖp swoim URL\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-azure-openai-key\"  # ZastƒÖp swoim kluczem API\n",
    "\n",
    "# Weryfikacja konfiguracji\n",
    "def test_openai_connection():\n",
    "    try:\n",
    "        # Inicjalizacja klienta OpenAI\n",
    "        openai.api_type = os.environ[\"OPENAI_API_TYPE\"]\n",
    "        openai.api_base = os.environ[\"OPENAI_API_BASE\"]\n",
    "        openai.api_version = os.environ[\"OPENAI_API_VERSION\"]\n",
    "        openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "        \n",
    "        # Pr√≥ba wykonania prostego zapytania\n",
    "        response = openai.Completion.create(\n",
    "            engine=\"text-davinci-003\",  # Dostosuj do dostƒôpnego modelu\n",
    "            prompt=\"Hello, world!\",\n",
    "            max_tokens=5\n",
    "        )\n",
    "        print(\"Po≈ÇƒÖczenie z Azure OpenAI dzia≈Ça poprawnie\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"B≈ÇƒÖd po≈ÇƒÖczenia z Azure OpenAI: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Uruchom test po≈ÇƒÖczenia\n",
    "connection_ok = test_openai_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afce165",
   "metadata": {},
   "source": [
    "### üîß Zadanie dla uczestnika: Generowanie embeddings\n",
    "U≈ºyj modelu embeddingowego z Azure OpenAI do przetworzenia wczytywanych dokument√≥w na wektory. \n",
    "\n",
    "**Przyk≈Çad:**\n",
    "```python\n",
    "response = openai.Embedding.create(\n",
    "    input=documents,\n",
    "    engine=\"text-embedding-ada-002\"\n",
    ")\n",
    "embeddings = [e[\"embedding\"] for e in response[\"data\"]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Databricks to platforma analityczna oparta na Apache Spark. Oferuje zintegrowane ≈õrodowisko do analizy danych, uczenia maszynowego i wizualizacji.\",\n",
    "    \"Azure OpenAI Service udostƒôpnia modele GPT-4, GPT-3.5 Turbo i inne w chmurze Microsoft Azure. Zapewnia zaawansowane funkcje przetwarzania jƒôzyka naturalnego.\",\n",
    "    \"Retrieval Augmented Generation (RAG) to technika ≈ÇƒÖczƒÖca wyszukiwanie informacji z generacjƒÖ tekstu. Pozwala na wykorzystanie zewnƒôtrznych ≈∫r√≥de≈Ç wiedzy w modelach generatywnych.\",\n",
    "    \"Microsoft Fabric to zintegrowana platforma analityczna, kt√≥ra ≈ÇƒÖczy r√≥≈ºne us≈Çugi analityczne w jednym miejscu. Obejmuje Data Engineering, Data Factory, Synapse Data Science i inne.\",\n",
    "    \"Wektorowe bazy danych, takie jak FAISS (Facebook AI Similarity Search), umo≈ºliwiajƒÖ efektywne przechowywanie i wyszukiwanie wektor√≥w embeddingowych reprezentujƒÖcych teksty lub obrazy.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = \"https://aissqldayedbworkshop002.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2023-05-15\"\n",
    "api_key = \"BCg7xEV4SUnG4QKIKi6rEYEnlZbzKzbpynNDh4XM1QX1BKDzJ6pfJQQJ99BEAC5RqLJXJ3w3AAAAACOGlwwk\"\n",
    "\n",
    "client = openai.AzureOpenAI(\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=api_key,\n",
    "    api_version=\"2023-09-01-preview\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "response = client.embeddings.create(\n",
    "    input=documents,\n",
    "    model=\"text-embedding-ada-002\"\n",
    ")\n",
    "#embeddings = [e[\"embedding\"] for e in response[\"data\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646bb201",
   "metadata": {},
   "source": [
    "### üîß Zadanie dla uczestnika: Wyszukiwanie wektorowe\n",
    "Zaimplementuj funkcjƒô, kt√≥ra przyjmie zapytanie u≈ºytkownika, wygeneruje jego embedding i znajdzie najbardziej podobne dokumenty przy u≈ºyciu metryki kosinusowej.\n",
    "\n",
    "**Przyk≈Çad:**\n",
    "```python\n",
    "# Funkcja por√≥wnujƒÖca wektory\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "query = \"Jak dzia≈Ça MLflow?\"\n",
    "query_emb = embedding_function(query)\n",
    "similarities = cosine_similarity([query_emb], embeddings)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e50fa87",
   "metadata": {},
   "source": [
    "### üîß Zadanie dla uczestnika: Generowanie odpowiedzi z LLM\n",
    "Wygeneruj prompt ≈ÇƒÖczƒÖcy kontekst znalezionych dokument√≥w z zapytaniem u≈ºytkownika i prze≈õlij go do Azure OpenAI.\n",
    "\n",
    "**Przyk≈Çad:**\n",
    "```python\n",
    "prompt = f\"Odpowiedz na pytanie w oparciu o poni≈ºszy kontekst:\\n{top_docs}\\n\\nPytanie: {query}\"\n",
    "response = openai.ChatCompletion.create(\n",
    "    engine=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przyk≈Çadowe dane testowe\n",
    "sample_documents = [\n",
    "    \"Databricks to platforma analityczna oparta na Apache Spark. Oferuje zintegrowane ≈õrodowisko do analizy danych, uczenia maszynowego i wizualizacji.\",\n",
    "    \"Azure OpenAI Service udostƒôpnia modele GPT-4, GPT-3.5 Turbo i inne w chmurze Microsoft Azure. Zapewnia zaawansowane funkcje przetwarzania jƒôzyka naturalnego.\",\n",
    "    \"Retrieval Augmented Generation (RAG) to technika ≈ÇƒÖczƒÖca wyszukiwanie informacji z generacjƒÖ tekstu. Pozwala na wykorzystanie zewnƒôtrznych ≈∫r√≥de≈Ç wiedzy w modelach generatywnych.\",\n",
    "    \"Microsoft Fabric to zintegrowana platforma analityczna, kt√≥ra ≈ÇƒÖczy r√≥≈ºne us≈Çugi analityczne w jednym miejscu. Obejmuje Data Engineering, Data Factory, Synapse Data Science i inne.\",\n",
    "    \"Wektorowe bazy danych, takie jak FAISS (Facebook AI Similarity Search), umo≈ºliwiajƒÖ efektywne przechowywanie i wyszukiwanie wektor√≥w embeddingowych reprezentujƒÖcych teksty lub obrazy.\"\n",
    "]\n",
    "\n",
    "# Przyk≈Çadowe dane tabelaryczne w formacie DataFrame\n",
    "sample_structured_data = pd.DataFrame({\n",
    "    'Product': ['Laptop', 'Smartphone', 'Tablet', 'Monitor', 'Keyboard'],\n",
    "    'Price': [5000, 3000, 2000, 1500, 300],\n",
    "    'Inventory': [120, 200, 80, 50, 300],\n",
    "    'Category': ['Electronics', 'Electronics', 'Electronics', 'Accessories', 'Accessories'],\n",
    "    'Description': [\n",
    "        'Wydajny laptop do zastosowa≈Ñ biznesowych z procesorem Intel i7',\n",
    "        'Smartfon z ekranem dotykowym i potr√≥jnym aparatem',\n",
    "        'Lekki tablet z d≈Çugim czasem pracy na baterii',\n",
    "        'Monitor 4K z wysokƒÖ czƒôstotliwo≈õciƒÖ od≈õwie≈ºania',\n",
    "        'Ergonomiczna klawiatura mechaniczna'\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Wy≈õwietlenie przyk≈Çadowych danych\n",
    "print(\"Przyk≈Çadowe dokumenty:\")\n",
    "for i, doc in enumerate(sample_documents):\n",
    "    print(f\"[{i}] {doc[:100]}...\")\n",
    "\n",
    "print(\"\\nPrzyk≈Çadowe dane strukturalne:\")\n",
    "display(sample_structured_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0269e94b",
   "metadata": {},
   "source": [
    "### üîß Zadanie dla uczestnika: Mechanizm pamiƒôci (memory)\n",
    "Utw√≥rz prostƒÖ strukturƒô przechowujƒÖcƒÖ historiƒô rozmowy (np. listƒô s≈Çownik√≥w). Do≈ÇƒÖczaj jƒÖ do promptu, aby model mia≈Ç kontekst wcze≈õniejszych pyta≈Ñ i odpowiedzi.\n",
    "\n",
    "**Przyk≈Çad:**\n",
    "```python\n",
    "memory = [\n",
    "    {\"role\": \"user\", \"content\": \"Jak dzia≈Ça Databricks?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Databricks to platforma analityczna oparta na Apache Spark...\"}\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66db3095",
   "metadata": {},
   "source": [
    "### üîß Zadanie dla uczestnika: Integracja ca≈Çego pipeline'u\n",
    "Po≈ÇƒÖcz wszystkie komponenty w jednƒÖ funkcjƒô lub klasƒô DataBota. Przetestuj jƒÖ na kilku r√≥≈ºnych zapytaniach i obserwuj odpowiedzi modelu.\n",
    "\n",
    "**Przyk≈Çad:**\n",
    "```python\n",
    "def databot_respond(query, memory):\n",
    "    # implementacja pipeline: embedding ‚Üí wyszukiwanie ‚Üí prompt ‚Üí odpowied≈∫\n",
    "    pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implementacja wyszukiwania wektorowego\n",
    "\n",
    "Teraz zaimplementujemy funkcjƒô wyszukiwania semantycznego, kt√≥ra bƒôdzie wykorzystywaƒá bazƒô wektorowƒÖ do znalezienia najbardziej odpowiednich fragment√≥w dokument√≥w dla danego zapytania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementacja funkcji wyszukiwania wektorowego\n",
    "def semantic_search(query, vectordb, top_k=3):\n",
    "    # Wykonaj wyszukiwanie wektorowe\n",
    "    results = vectordb.similarity_search_with_score(query, k=top_k)\n",
    "    \n",
    "    # Formatuj wyniki\n",
    "    formatted_results = []\n",
    "    for doc, score in results:\n",
    "        formatted_results.append({\n",
    "            \"text\": doc.page_content,\n",
    "            \"source\": doc.metadata.get(\"source\", \"unknown\"),\n",
    "            \"similarity_score\": float(score)\n",
    "        })\n",
    "    \n",
    "    return formatted_results\n",
    "\n",
    "# Testowanie funkcji wyszukiwania\n",
    "test_query = \"Czym jest RAG i jak dzia≈Ça?\"\n",
    "try:\n",
    "    search_results = semantic_search(test_query, vectordb)\n",
    "    print(f\"Wyniki wyszukiwania dla zapytania: '{test_query}'\\n\")\n",
    "    for i, result in enumerate(search_results):\n",
    "        print(f\"Wynik {i+1} (score: {result['similarity_score']:.4f})\")\n",
    "        print(f\"≈πr√≥d≈Ço: {result['source']}\")\n",
    "        print(f\"Tekst: {result['text']}\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"B≈ÇƒÖd podczas wyszukiwania: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Integracja z Azure OpenAI do generowania odpowiedzi\n",
    "\n",
    "Teraz zintegrujemy nasze wyszukiwanie semantyczne z modelem LLM, aby generowaƒá odpowiedzi na podstawie znalezionych fragment√≥w dokument√≥w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementacja funkcji generowania odpowiedzi przy u≈ºyciu Azure OpenAI\n",
    "def generate_response(query, search_results):\n",
    "    # Inicjalizacja modelu LLM\n",
    "    llm = AzureOpenAI(\n",
    "        deployment_name=\"gpt-4\",  # Dostosuj do dostƒôpnego modelu\n",
    "        model_name=\"gpt-4\",\n",
    "        openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "        openai_api_base=os.environ[\"OPENAI_API_BASE\"],\n",
    "        openai_api_version=os.environ[\"OPENAI_API_VERSION\"]\n",
    "    )\n",
    "    \n",
    "    # Przygotuj kontekst z wynik√≥w wyszukiwania\n",
    "    context = \"\\n\\n\".join([f\"≈πr√≥d≈Ço: {r['source']}\\nTekst: {r['text']}\" for r in search_results])\n",
    "    \n",
    "    # Przygotuj szablon promptu\n",
    "    template = \"\"\"\n",
    "    Odpowiedz na poni≈ºsze pytanie, korzystajƒÖc tylko z informacji zawartych w dostarczonym kontek≈õcie.\n",
    "    Je≈õli nie mo≈ºesz znale≈∫ƒá odpowiedzi w kontek≈õcie, powiedz \"Nie znalaz≈Çem odpowiedzi w dostƒôpnych danych\".\n",
    "    \n",
    "    Kontekst:\n",
    "    {context}\n",
    "    \n",
    "    Pytanie: {query}\n",
    "    \n",
    "    Odpowied≈∫:\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"context\", \"query\"])\n",
    "    \n",
    "    # Utw√≥rz ≈Ça≈Ñcuch LLM\n",
    "    chain = LLMChain(prompt=prompt, llm=llm)\n",
    "    \n",
    "    # Generuj odpowied≈∫\n",
    "    response = chain.run(context=context, query=query)\n",
    "    \n",
    "    return response.strip()\n",
    "\n",
    "# Testowanie funkcji generowania odpowiedzi\n",
    "try:\n",
    "    response = generate_response(test_query, search_results)\n",
    "    print(f\"Odpowied≈∫ na pytanie: '{test_query}'\\n\")\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(f\"B≈ÇƒÖd podczas generowania odpowiedzi: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Implementacja prostego mechanizmu ≈õledzenia rozmowy (memory)\n",
    "\n",
    "Aby DataBot m√≥g≈Ç prowadziƒá sp√≥jnƒÖ konwersacjƒô, musimy zaimplementowaƒá mechanizm ≈õledzenia rozmowy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementacja prostego mechanizmu ≈õledzenia rozmowy\n",
    "class ConversationMemory:\n",
    "    def __init__(self, max_history=5):\n",
    "        self.conversation_history = []\n",
    "        self.max_history = max_history\n",
    "    \n",
    "    def add_interaction(self, query, response):\n",
    "        self.conversation_history.append({\"query\": query, \"response\": response, \"timestamp\": time.time()})\n",
    "        # Ogranicz historiƒô do max_history ostatnich interakcji\n",
    "        if len(self.conversation_history) > self.max_history:\n",
    "            self.conversation_history = self.conversation_history[-self.max_history:]\n",
    "    \n",
    "    def get_history(self):\n",
    "        return self.conversation_history\n",
    "    \n",
    "    def get_formatted_history(self):\n",
    "        formatted = \"\"\n",
    "        for interaction in self.conversation_history:\n",
    "            formatted += f\"User: {interaction['query']}\\n\"\n",
    "            formatted += f\"Assistant: {interaction['response']}\\n\\n\"\n",
    "        return formatted\n",
    "    \n",
    "    def clear_history(self):\n",
    "        self.conversation_history = []\n",
    "\n",
    "# Inicjalizacja pamiƒôci konwersacji\n",
    "memory = ConversationMemory()\n",
    "\n",
    "# Dodanie pierwszej interakcji do pamiƒôci\n",
    "memory.add_interaction(test_query, response)\n",
    "\n",
    "# Wy≈õwietlenie historii konwersacji\n",
    "print(\"Historia konwersacji:\")\n",
    "print(memory.get_formatted_history())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Integracja wszystkich komponent√≥w w kompletnym pipeline'ie\n",
    "\n",
    "Teraz po≈ÇƒÖczymy wszystkie komponenty w jeden kompletny pipeline DataBota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integracja wszystkich komponent√≥w w klasie DataBot\n",
    "class DataBot:\n",
    "    def __init__(self, documents=None, structured_data=None):\n",
    "        # Inicjalizacja pamiƒôci konwersacji\n",
    "        self.memory = ConversationMemory()\n",
    "        \n",
    "        # Inicjalizacja bazy wektorowej dla dokument√≥w\n",
    "        if documents:\n",
    "            self.vectordb = generate_embeddings(documents)\n",
    "        else:\n",
    "            self.vectordb = None\n",
    "        \n",
    "        # Przechowywanie danych strukturalnych\n",
    "        self.structured_data = structured_data\n",
    "        \n",
    "        print(\"DataBot zosta≈Ç zainicjalizowany\")\n",
    "    \n",
    "    def process_query(self, query, use_memory=True, top_k=3):\n",
    "        # Rozszerzenie zapytania o kontekst z historii konwersacji, je≈õli w≈ÇƒÖczone\n",
    "        if use_memory and self.memory.get_history():\n",
    "            context_query = f\"BiorƒÖc pod uwagƒô poprzedniƒÖ konwersacjƒô:\\n{self.memory.get_formatted_history()}\\nOdpowiedz na pytanie: {query}\"\n",
    "        else:\n",
    "            context_query = query\n",
    "        \n",
    "        # Wyszukiwanie semantyczne w dokumentach\n",
    "        if self.vectordb:\n",
    "            search_results = semantic_search(query, self.vectordb, top_k=top_k)\n",
    "        else:\n",
    "            search_results = []\n",
    "        \n",
    "        # Dodanie danych strukturalnych, je≈õli pytanie mo≈ºe ich dotyczyƒá\n",
    "        # To jest uproszczona implementacja - w rzeczywisto≈õci potrzebny by≈Çby bardziej zaawansowany mechanizm\n",
    "        if self.structured_data is not None and any(keyword in query.lower() for keyword in [\"produkt\", \"cena\", \"inventory\", \"kategoria\"]):\n",
    "            # Konwersja DataFrame na tekstowƒÖ reprezentacjƒô\n",
    "            structured_text = self.structured_data.to_string()\n",
    "            search_results.append({\n",
    "                \"text\": f\"Dane produktowe:\\n{structured_text}\",\n",
    "                \"source\": \"structured_data\",\n",
    "                \"similarity_score\": 1.0  # Przypisujemy wysoki wynik, poniewa≈º jawnie w≈ÇƒÖczamy te dane\n",
    "            })\n",
    "        \n",
    "        # Generowanie odpowiedzi\n",
    "        response = generate_response(context_query, search_results)\n",
    "        \n",
    "        # Dodanie interakcji do pamiƒôci\n",
    "        self.memory.add_interaction(query, response)\n",
    "        \n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"search_results\": search_results,\n",
    "            \"response\": response\n",
    "        }\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        self.memory.clear_history()\n",
    "        print(\"Historia konwersacji zosta≈Ça wyczyszczona\")\n",
    "\n",
    "# Inicjalizacja DataBota z naszymi przyk≈Çadowymi danymi\n",
    "databot = DataBot(documents=sample_documents, structured_data=sample_structured_data)\n",
    "\n",
    "# Testowanie DataBota\n",
    "test_queries = [\n",
    "    \"Czym jest RAG i jak to pomaga w AI?\",\n",
    "    \"Kt√≥re produkty sƒÖ dostƒôpne w kategorii Electronics?\",\n",
    "    \"Mo≈ºesz powiedzieƒá wiƒôcej o Microsoft Fabric?\"\n",
    "]\n",
    "\n",
    "for i, query in enumerate(test_queries):\n",
    "    print(f\"\\n--- Zapytanie {i+1}: {query} ---\")\n",
    "    result = databot.process_query(query)\n",
    "    print(\"\\nOdpowied≈∫:\")\n",
    "    print(result[\"response\"])\n",
    "\n",
    "print(\"\\n--- Historia konwersacji ---\")\n",
    "print(databot.memory.get_formatted_history())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Zadania do wykonania\n",
    "\n",
    "1. Zmodyfikuj promptu dla LLM, aby poprawiƒá jako≈õƒá generowanych odpowiedzi\n",
    "2. Dodaj obs≈Çugƒô odpowiedzi na pytania, na kt√≥re nie ma informacji w dostƒôpnych danych\n",
    "3. Zaimplementuj mechanizm filtrowania dokument√≥w na podstawie ich wynik√≥w podobie≈Ñstwa (np. ustaw pr√≥g minimalnego podobie≈Ñstwa)\n",
    "4. Dodaj obs≈Çugƒô zapyta≈Ñ follow-up, kt√≥re odnoszƒÖ siƒô do wcze≈õniejszych odpowiedzi\n",
    "\n",
    "## Zadania dodatkowe\n",
    "\n",
    "Je≈õli masz wiƒôcej czasu, mo≈ºesz spr√≥bowaƒá:\n",
    "\n",
    "1. Zaimplementowanie mechanizmu oceny jako≈õci odpowiedzi (evaluation)\n",
    "2. Dodanie obs≈Çugi r√≥≈ºnych typ√≥w zapyta≈Ñ (np. zapytania o dane, zapytania o wiedzƒô og√≥lnƒÖ, zapytania o akcje)\n",
    "3. Implementacja bardziej zaawansowanego promptu dla LLM z wykorzystaniem Chain of Thought\n",
    "4. Zaimplementowanie mechanizmu wyja≈õnialno≈õci odpowiedzi (np. podanie ≈∫r√≥de≈Ç informacji)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
