{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ćwiczenie: Budowa core DataBota\n",
    "\n",
    "**Czas trwania:** 90 minut\n",
    "\n",
    "**Cel ćwiczenia:** W tym ćwiczeniu stworzymy szkielet DataBota, który będzie w stanie przetwarzać zapytania użytkownika, pobierać odpowiednie dane i generować odpowiedzi przy użyciu modelu LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Wprowadzenie do przetwarzania zapytań użytkownika\n",
    "\n",
    "Nasza architektura DataBota będzie oparta na następującym pipeline przetwarzania zapytań:\n",
    "\n",
    "1. Użytkownik wprowadza zapytanie w języku naturalnym\n",
    "2. Zapytanie jest konwertowane na wektor embeddings\n",
    "3. System wykonuje wyszukiwanie semantyczne w bazie danych wektorowej\n",
    "4. Znalezione informacje są przekazywane jako kontekst do modelu LLM\n",
    "5. Model LLM generuje odpowiedź na podstawie zapytania i kontekstu\n",
    "\n",
    "Ta architektura, znana jako Retrieval Augmented Generation (RAG), pozwala na wykorzystanie zewnętrznych źródeł danych w procesie generowania odpowiedzi, co znacznie zwiększa dokładność i aktualność informacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalacja wymaganych bibliotek\n",
    "%pip install openai langchain tiktoken pymupdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Import bibliotek\n",
    "import openai\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from langchain import LLMChain\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Sprawdzenie, czy wszystkie potrzebne biblioteki zostały poprawnie zaimportowane\n",
    "print(\"Środowisko zostało zainicjalizowane pomyślnie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Konfiguracja dostępu do Azure OpenAI\n",
    "\n",
    "Aby korzystać z modeli OpenAI, musimy skonfigurować dostęp do Azure OpenAI Service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "endpoint = \"https://aissqldayedbworkshop001.openai.azure.com/\"\n",
    "model_name = \"o3-mini\"\n",
    "deployment = \"o3-mini\"\n",
    "\n",
    "subscription_key=\"4mE2kj9PLeZ0NjqMiFzgxtStKtJIDRnZ4dzNIsipygEDdbYHmlCXJQQJ99BEAC5RqLJXJ3w3AAAAACOG85N1\"\n",
    "api_version = \"2024-12-01-preview\"\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_version=api_version,\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=subscription_key,\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"I am going to Paris, what should I see?\",\n",
    "        }\n",
    "    ],\n",
    "    max_completion_tokens=100000,\n",
    "    model=deployment\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konfiguracja dostępu do Azure OpenAI\n",
    "# W środowisku produkcyjnym te dane powinny być przechowywane w Azure Key Vault lub Databricks Secrets\n",
    "\n",
    "# Dla Azure OpenAI Service\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2023-05-15\"  # Dostosuj do aktualnej wersji API\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://your-azure-openai-resource.openai.azure.com/\"  # Zastąp swoim URL\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-azure-openai-key\"  # Zastąp swoim kluczem API\n",
    "\n",
    "# Weryfikacja konfiguracji\n",
    "def test_openai_connection():\n",
    "    try:\n",
    "        # Inicjalizacja klienta OpenAI\n",
    "        openai.api_type = os.environ[\"OPENAI_API_TYPE\"]\n",
    "        openai.api_base = os.environ[\"OPENAI_API_BASE\"]\n",
    "        openai.api_version = os.environ[\"OPENAI_API_VERSION\"]\n",
    "        openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "        \n",
    "        # Próba wykonania prostego zapytania\n",
    "        response = openai.Completion.create(\n",
    "            engine=\"text-davinci-003\",  # Dostosuj do dostępnego modelu\n",
    "            prompt=\"Hello, world!\",\n",
    "            max_tokens=5\n",
    "        )\n",
    "        print(\"Połączenie z Azure OpenAI działa poprawnie\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Błąd połączenia z Azure OpenAI: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Uruchom test połączenia\n",
    "connection_ok = test_openai_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afce165",
   "metadata": {},
   "source": [
    "### 🔧 Zadanie dla uczestnika: Generowanie embeddings\n",
    "Użyj modelu embeddingowego z Azure OpenAI do przetworzenia wczytywanych dokumentów na wektory. \n",
    "\n",
    "**Przykład:**\n",
    "```python\n",
    "response = openai.Embedding.create(\n",
    "    input=documents,\n",
    "    engine=\"text-embedding-ada-002\"\n",
    ")\n",
    "embeddings = [e[\"embedding\"] for e in response[\"data\"]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Databricks to platforma analityczna oparta na Apache Spark. Oferuje zintegrowane środowisko do analizy danych, uczenia maszynowego i wizualizacji.\",\n",
    "    \"Azure OpenAI Service udostępnia modele GPT-4, GPT-3.5 Turbo i inne w chmurze Microsoft Azure. Zapewnia zaawansowane funkcje przetwarzania języka naturalnego.\",\n",
    "    \"Retrieval Augmented Generation (RAG) to technika łącząca wyszukiwanie informacji z generacją tekstu. Pozwala na wykorzystanie zewnętrznych źródeł wiedzy w modelach generatywnych.\",\n",
    "    \"Microsoft Fabric to zintegrowana platforma analityczna, która łączy różne usługi analityczne w jednym miejscu. Obejmuje Data Engineering, Data Factory, Synapse Data Science i inne.\",\n",
    "    \"Wektorowe bazy danych, takie jak FAISS (Facebook AI Similarity Search), umożliwiają efektywne przechowywanie i wyszukiwanie wektorów embeddingowych reprezentujących teksty lub obrazy.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = \"https://aissqldayedbworkshop002.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2023-05-15\"\n",
    "api_key = \"BCg7xEV4SUnG4QKIKi6rEYEnlZbzKzbpynNDh4XM1QX1BKDzJ6pfJQQJ99BEAC5RqLJXJ3w3AAAAACOGlwwk\"\n",
    "\n",
    "client = openai.AzureOpenAI(\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=api_key,\n",
    "    api_version=\"2023-09-01-preview\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "response = client.embeddings.create(\n",
    "    input=documents,\n",
    "    model=\"text-embedding-ada-002\"\n",
    ")\n",
    "#embeddings = [e[\"embedding\"] for e in response[\"data\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646bb201",
   "metadata": {},
   "source": [
    "### 🔧 Zadanie dla uczestnika: Wyszukiwanie wektorowe\n",
    "Zaimplementuj funkcję, która przyjmie zapytanie użytkownika, wygeneruje jego embedding i znajdzie najbardziej podobne dokumenty przy użyciu metryki kosinusowej.\n",
    "\n",
    "**Przykład:**\n",
    "```python\n",
    "# Funkcja porównująca wektory\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "query = \"Jak działa MLflow?\"\n",
    "query_emb = embedding_function(query)\n",
    "similarities = cosine_similarity([query_emb], embeddings)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e50fa87",
   "metadata": {},
   "source": [
    "### 🔧 Zadanie dla uczestnika: Generowanie odpowiedzi z LLM\n",
    "Wygeneruj prompt łączący kontekst znalezionych dokumentów z zapytaniem użytkownika i prześlij go do Azure OpenAI.\n",
    "\n",
    "**Przykład:**\n",
    "```python\n",
    "prompt = f\"Odpowiedz na pytanie w oparciu o poniższy kontekst:\\n{top_docs}\\n\\nPytanie: {query}\"\n",
    "response = openai.ChatCompletion.create(\n",
    "    engine=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykładowe dane testowe\n",
    "sample_documents = [\n",
    "    \"Databricks to platforma analityczna oparta na Apache Spark. Oferuje zintegrowane środowisko do analizy danych, uczenia maszynowego i wizualizacji.\",\n",
    "    \"Azure OpenAI Service udostępnia modele GPT-4, GPT-3.5 Turbo i inne w chmurze Microsoft Azure. Zapewnia zaawansowane funkcje przetwarzania języka naturalnego.\",\n",
    "    \"Retrieval Augmented Generation (RAG) to technika łącząca wyszukiwanie informacji z generacją tekstu. Pozwala na wykorzystanie zewnętrznych źródeł wiedzy w modelach generatywnych.\",\n",
    "    \"Microsoft Fabric to zintegrowana platforma analityczna, która łączy różne usługi analityczne w jednym miejscu. Obejmuje Data Engineering, Data Factory, Synapse Data Science i inne.\",\n",
    "    \"Wektorowe bazy danych, takie jak FAISS (Facebook AI Similarity Search), umożliwiają efektywne przechowywanie i wyszukiwanie wektorów embeddingowych reprezentujących teksty lub obrazy.\"\n",
    "]\n",
    "\n",
    "# Przykładowe dane tabelaryczne w formacie DataFrame\n",
    "sample_structured_data = pd.DataFrame({\n",
    "    'Product': ['Laptop', 'Smartphone', 'Tablet', 'Monitor', 'Keyboard'],\n",
    "    'Price': [5000, 3000, 2000, 1500, 300],\n",
    "    'Inventory': [120, 200, 80, 50, 300],\n",
    "    'Category': ['Electronics', 'Electronics', 'Electronics', 'Accessories', 'Accessories'],\n",
    "    'Description': [\n",
    "        'Wydajny laptop do zastosowań biznesowych z procesorem Intel i7',\n",
    "        'Smartfon z ekranem dotykowym i potrójnym aparatem',\n",
    "        'Lekki tablet z długim czasem pracy na baterii',\n",
    "        'Monitor 4K z wysoką częstotliwością odświeżania',\n",
    "        'Ergonomiczna klawiatura mechaniczna'\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Wyświetlenie przykładowych danych\n",
    "print(\"Przykładowe dokumenty:\")\n",
    "for i, doc in enumerate(sample_documents):\n",
    "    print(f\"[{i}] {doc[:100]}...\")\n",
    "\n",
    "print(\"\\nPrzykładowe dane strukturalne:\")\n",
    "display(sample_structured_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0269e94b",
   "metadata": {},
   "source": [
    "### 🔧 Zadanie dla uczestnika: Mechanizm pamięci (memory)\n",
    "Utwórz prostą strukturę przechowującą historię rozmowy (np. listę słowników). Dołączaj ją do promptu, aby model miał kontekst wcześniejszych pytań i odpowiedzi.\n",
    "\n",
    "**Przykład:**\n",
    "```python\n",
    "memory = [\n",
    "    {\"role\": \"user\", \"content\": \"Jak działa Databricks?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Databricks to platforma analityczna oparta na Apache Spark...\"}\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66db3095",
   "metadata": {},
   "source": [
    "### 🔧 Zadanie dla uczestnika: Integracja całego pipeline'u\n",
    "Połącz wszystkie komponenty w jedną funkcję lub klasę DataBota. Przetestuj ją na kilku różnych zapytaniach i obserwuj odpowiedzi modelu.\n",
    "\n",
    "**Przykład:**\n",
    "```python\n",
    "def databot_respond(query, memory):\n",
    "    # implementacja pipeline: embedding → wyszukiwanie → prompt → odpowiedź\n",
    "    pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implementacja wyszukiwania wektorowego\n",
    "\n",
    "Teraz zaimplementujemy funkcję wyszukiwania semantycznego, która będzie wykorzystywać bazę wektorową do znalezienia najbardziej odpowiednich fragmentów dokumentów dla danego zapytania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementacja funkcji wyszukiwania wektorowego\n",
    "def semantic_search(query, vectordb, top_k=3):\n",
    "    # Wykonaj wyszukiwanie wektorowe\n",
    "    results = vectordb.similarity_search_with_score(query, k=top_k)\n",
    "    \n",
    "    # Formatuj wyniki\n",
    "    formatted_results = []\n",
    "    for doc, score in results:\n",
    "        formatted_results.append({\n",
    "            \"text\": doc.page_content,\n",
    "            \"source\": doc.metadata.get(\"source\", \"unknown\"),\n",
    "            \"similarity_score\": float(score)\n",
    "        })\n",
    "    \n",
    "    return formatted_results\n",
    "\n",
    "# Testowanie funkcji wyszukiwania\n",
    "test_query = \"Czym jest RAG i jak działa?\"\n",
    "try:\n",
    "    search_results = semantic_search(test_query, vectordb)\n",
    "    print(f\"Wyniki wyszukiwania dla zapytania: '{test_query}'\\n\")\n",
    "    for i, result in enumerate(search_results):\n",
    "        print(f\"Wynik {i+1} (score: {result['similarity_score']:.4f})\")\n",
    "        print(f\"Źródło: {result['source']}\")\n",
    "        print(f\"Tekst: {result['text']}\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"Błąd podczas wyszukiwania: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Integracja z Azure OpenAI do generowania odpowiedzi\n",
    "\n",
    "Teraz zintegrujemy nasze wyszukiwanie semantyczne z modelem LLM, aby generować odpowiedzi na podstawie znalezionych fragmentów dokumentów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementacja funkcji generowania odpowiedzi przy użyciu Azure OpenAI\n",
    "def generate_response(query, search_results):\n",
    "    # Inicjalizacja modelu LLM\n",
    "    llm = AzureOpenAI(\n",
    "        deployment_name=\"gpt-4\",  # Dostosuj do dostępnego modelu\n",
    "        model_name=\"gpt-4\",\n",
    "        openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "        openai_api_base=os.environ[\"OPENAI_API_BASE\"],\n",
    "        openai_api_version=os.environ[\"OPENAI_API_VERSION\"]\n",
    "    )\n",
    "    \n",
    "    # Przygotuj kontekst z wyników wyszukiwania\n",
    "    context = \"\\n\\n\".join([f\"Źródło: {r['source']}\\nTekst: {r['text']}\" for r in search_results])\n",
    "    \n",
    "    # Przygotuj szablon promptu\n",
    "    template = \"\"\"\n",
    "    Odpowiedz na poniższe pytanie, korzystając tylko z informacji zawartych w dostarczonym kontekście.\n",
    "    Jeśli nie możesz znaleźć odpowiedzi w kontekście, powiedz \"Nie znalazłem odpowiedzi w dostępnych danych\".\n",
    "    \n",
    "    Kontekst:\n",
    "    {context}\n",
    "    \n",
    "    Pytanie: {query}\n",
    "    \n",
    "    Odpowiedź:\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"context\", \"query\"])\n",
    "    \n",
    "    # Utwórz łańcuch LLM\n",
    "    chain = LLMChain(prompt=prompt, llm=llm)\n",
    "    \n",
    "    # Generuj odpowiedź\n",
    "    response = chain.run(context=context, query=query)\n",
    "    \n",
    "    return response.strip()\n",
    "\n",
    "# Testowanie funkcji generowania odpowiedzi\n",
    "try:\n",
    "    response = generate_response(test_query, search_results)\n",
    "    print(f\"Odpowiedź na pytanie: '{test_query}'\\n\")\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(f\"Błąd podczas generowania odpowiedzi: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Implementacja prostego mechanizmu śledzenia rozmowy (memory)\n",
    "\n",
    "Aby DataBot mógł prowadzić spójną konwersację, musimy zaimplementować mechanizm śledzenia rozmowy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementacja prostego mechanizmu śledzenia rozmowy\n",
    "class ConversationMemory:\n",
    "    def __init__(self, max_history=5):\n",
    "        self.conversation_history = []\n",
    "        self.max_history = max_history\n",
    "    \n",
    "    def add_interaction(self, query, response):\n",
    "        self.conversation_history.append({\"query\": query, \"response\": response, \"timestamp\": time.time()})\n",
    "        # Ogranicz historię do max_history ostatnich interakcji\n",
    "        if len(self.conversation_history) > self.max_history:\n",
    "            self.conversation_history = self.conversation_history[-self.max_history:]\n",
    "    \n",
    "    def get_history(self):\n",
    "        return self.conversation_history\n",
    "    \n",
    "    def get_formatted_history(self):\n",
    "        formatted = \"\"\n",
    "        for interaction in self.conversation_history:\n",
    "            formatted += f\"User: {interaction['query']}\\n\"\n",
    "            formatted += f\"Assistant: {interaction['response']}\\n\\n\"\n",
    "        return formatted\n",
    "    \n",
    "    def clear_history(self):\n",
    "        self.conversation_history = []\n",
    "\n",
    "# Inicjalizacja pamięci konwersacji\n",
    "memory = ConversationMemory()\n",
    "\n",
    "# Dodanie pierwszej interakcji do pamięci\n",
    "memory.add_interaction(test_query, response)\n",
    "\n",
    "# Wyświetlenie historii konwersacji\n",
    "print(\"Historia konwersacji:\")\n",
    "print(memory.get_formatted_history())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Integracja wszystkich komponentów w kompletnym pipeline'ie\n",
    "\n",
    "Teraz połączymy wszystkie komponenty w jeden kompletny pipeline DataBota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integracja wszystkich komponentów w klasie DataBot\n",
    "class DataBot:\n",
    "    def __init__(self, documents=None, structured_data=None):\n",
    "        # Inicjalizacja pamięci konwersacji\n",
    "        self.memory = ConversationMemory()\n",
    "        \n",
    "        # Inicjalizacja bazy wektorowej dla dokumentów\n",
    "        if documents:\n",
    "            self.vectordb = generate_embeddings(documents)\n",
    "        else:\n",
    "            self.vectordb = None\n",
    "        \n",
    "        # Przechowywanie danych strukturalnych\n",
    "        self.structured_data = structured_data\n",
    "        \n",
    "        print(\"DataBot został zainicjalizowany\")\n",
    "    \n",
    "    def process_query(self, query, use_memory=True, top_k=3):\n",
    "        # Rozszerzenie zapytania o kontekst z historii konwersacji, jeśli włączone\n",
    "        if use_memory and self.memory.get_history():\n",
    "            context_query = f\"Biorąc pod uwagę poprzednią konwersację:\\n{self.memory.get_formatted_history()}\\nOdpowiedz na pytanie: {query}\"\n",
    "        else:\n",
    "            context_query = query\n",
    "        \n",
    "        # Wyszukiwanie semantyczne w dokumentach\n",
    "        if self.vectordb:\n",
    "            search_results = semantic_search(query, self.vectordb, top_k=top_k)\n",
    "        else:\n",
    "            search_results = []\n",
    "        \n",
    "        # Dodanie danych strukturalnych, jeśli pytanie może ich dotyczyć\n",
    "        # To jest uproszczona implementacja - w rzeczywistości potrzebny byłby bardziej zaawansowany mechanizm\n",
    "        if self.structured_data is not None and any(keyword in query.lower() for keyword in [\"produkt\", \"cena\", \"inventory\", \"kategoria\"]):\n",
    "            # Konwersja DataFrame na tekstową reprezentację\n",
    "            structured_text = self.structured_data.to_string()\n",
    "            search_results.append({\n",
    "                \"text\": f\"Dane produktowe:\\n{structured_text}\",\n",
    "                \"source\": \"structured_data\",\n",
    "                \"similarity_score\": 1.0  # Przypisujemy wysoki wynik, ponieważ jawnie włączamy te dane\n",
    "            })\n",
    "        \n",
    "        # Generowanie odpowiedzi\n",
    "        response = generate_response(context_query, search_results)\n",
    "        \n",
    "        # Dodanie interakcji do pamięci\n",
    "        self.memory.add_interaction(query, response)\n",
    "        \n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"search_results\": search_results,\n",
    "            \"response\": response\n",
    "        }\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        self.memory.clear_history()\n",
    "        print(\"Historia konwersacji została wyczyszczona\")\n",
    "\n",
    "# Inicjalizacja DataBota z naszymi przykładowymi danymi\n",
    "databot = DataBot(documents=sample_documents, structured_data=sample_structured_data)\n",
    "\n",
    "# Testowanie DataBota\n",
    "test_queries = [\n",
    "    \"Czym jest RAG i jak to pomaga w AI?\",\n",
    "    \"Które produkty są dostępne w kategorii Electronics?\",\n",
    "    \"Możesz powiedzieć więcej o Microsoft Fabric?\"\n",
    "]\n",
    "\n",
    "for i, query in enumerate(test_queries):\n",
    "    print(f\"\\n--- Zapytanie {i+1}: {query} ---\")\n",
    "    result = databot.process_query(query)\n",
    "    print(\"\\nOdpowiedź:\")\n",
    "    print(result[\"response\"])\n",
    "\n",
    "print(\"\\n--- Historia konwersacji ---\")\n",
    "print(databot.memory.get_formatted_history())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Zadania do wykonania\n",
    "\n",
    "1. Zmodyfikuj promptu dla LLM, aby poprawić jakość generowanych odpowiedzi\n",
    "2. Dodaj obsługę odpowiedzi na pytania, na które nie ma informacji w dostępnych danych\n",
    "3. Zaimplementuj mechanizm filtrowania dokumentów na podstawie ich wyników podobieństwa (np. ustaw próg minimalnego podobieństwa)\n",
    "4. Dodaj obsługę zapytań follow-up, które odnoszą się do wcześniejszych odpowiedzi\n",
    "\n",
    "## Zadania dodatkowe\n",
    "\n",
    "Jeśli masz więcej czasu, możesz spróbować:\n",
    "\n",
    "1. Zaimplementowanie mechanizmu oceny jakości odpowiedzi (evaluation)\n",
    "2. Dodanie obsługi różnych typów zapytań (np. zapytania o dane, zapytania o wiedzę ogólną, zapytania o akcje)\n",
    "3. Implementacja bardziej zaawansowanego promptu dla LLM z wykorzystaniem Chain of Thought\n",
    "4. Zaimplementowanie mechanizmu wyjaśnialności odpowiedzi (np. podanie źródeł informacji)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
