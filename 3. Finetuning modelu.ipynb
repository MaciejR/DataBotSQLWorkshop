{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ćwiczenie: Fine-tuning prostego modelu odpowiedzi + rejestracja w AI Foundry\n",
    "\n",
    "**Czas trwania:** 75 minut\n",
    "\n",
    "**Cel ćwiczenia:** W tym ćwiczeniu przygotujemy zbiór danych do fine-tuningu, przeprowadzimy proces dostrajania modelu LLM, zarejestrujemy model w Azure AI Foundry oraz porównamy wyniki modelu podstawowego i dostrojonego."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Wprowadzenie do fine-tuningu modeli LLM\n",
    "\n",
    "Fine-tuning (dostrajanie) modeli językowych to proces adaptacji wstępnie wytrenowanego modelu do specyficznego zadania lub domeny. Podczas gdy modele podstawowe, takie jak GPT-4 czy Llama 2, posiadają szeroką wiedzę ogólną, mogą nie być zoptymalizowane do konkretnych zastosowań w organizacji.\n",
    "\n",
    "Korzyści z fine-tuningu:\n",
    "- Dostosowanie modelu do specyficznego słownictwa i kontekstu organizacji\n",
    "- Poprawa jakości odpowiedzi dla specjalistycznych zapytań\n",
    "- Zmniejszenie \"halucynacji\" poprzez dostosowanie do faktów specyficznych dla organizacji\n",
    "- Potencjalne zmniejszenie wielkości modelu przy zachowaniu jakości dla konkretnych zadań\n",
    "\n",
    "W tym ćwiczeniu skupimy się na metodzie LoRA (Low-Rank Adaptation), która pozwala na efektywne dostrajanie dużych modeli bez konieczności modyfikowania wszystkich parametrów modelu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Przygotowanie zbioru danych do fine-tuningu\n",
    "\n",
    "Do fine-tuningu potrzebujemy zestawu par prompt-odpowiedź, które są specyficzne dla naszej domeny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przygotowanie przykładowego zbioru danych do fine-tuningu\n",
    "# W rzeczywistym scenariuszu powinien to być specyficzny dla domeny zestaw pytań i odpowiedzi\n",
    "\n",
    "# Przykładowy zestaw danych - pytania i odpowiedzi specyficzne dla organizacji\n",
    "training_data = [\n",
    "    {\n",
    "        \"prompt\": \"Co to jest Microsoft Fabric?\",\n",
    "        \"completion\": \"Microsoft Fabric to zintegrowana platforma analityczna, która łączy różne usługi analityczne w jednym miejscu. Obejmuje Data Engineering, Data Factory, Synapse Data Science, Data Warehouse, Real-Time Analytics i Power BI. Fabric zapewnia zunifikowane środowisko do pracy z danymi, od inżynierii danych po raportowanie i analizę biznesową.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Jak skonfigurować dostęp do Azure OpenAI w aplikacji DataBot?\",\n",
    "        \"completion\": \"Aby skonfigurować dostęp do Azure OpenAI w aplikacji DataBot, musisz wykonać następujące kroki: 1) Utwórz zasób Azure OpenAI w portalu Azure, 2) Uzyskaj klucz API i endpoint, 3) Skonfiguruj zmienne środowiskowe OPENAI_API_KEY, OPENAI_API_BASE, OPENAI_API_TYPE i OPENAI_API_VERSION, 4) W kodzie aplikacji użyj tych zmiennych do inicjalizacji klienta OpenAI. Pamiętaj, żeby przechowywać klucze API w bezpiecznym miejscu, na przykład w Azure Key Vault lub Databricks Secrets.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Jakie są zalety używania Databricks Lakehouse?\",\n",
    "        \"completion\": \"Databricks Lakehouse oferuje wiele zalet: 1) Łączy możliwości hurtowni danych (strukturę, transakcje i jakość danych) z elastycznością jezior danych, 2) Zapewnia skalowalność i wydajność dzięki silnikowi Spark, 3) Umożliwia pracę z danymi strukturalnymi i niestrukturalnymi, 4) Oferuje zintegrowane narzędzia do uczenia maszynowego i analityki, 5) Zapewnia wsparcie dla różnych języków jak SQL, Python, R i Scala, 6) Umożliwia współpracę wielu użytkowników w czasie rzeczywistym, 7) Oferuje zaawansowane mechanizmy bezpieczeństwa i zarządzania dostępem do danych.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Jak działa architektura RAG w kontekście naszej organizacji?\",\n",
    "        \"completion\": \"W naszej organizacji, architektura RAG (Retrieval Augmented Generation) działa w następujący sposób: 1) Dane z różnych źródeł organizacyjnych (SharePoint, SQL, Data Lake) są indeksowane i przechowywane w bazie wektorowej, 2) Gdy użytkownik zadaje pytanie, system wyszukuje najbardziej odpowiednie fragmenty dokumentów, 3) Znalezione fragmenty są przekazywane do modelu LLM jako kontekst razem z pytaniem użytkownika, 4) Model generuje odpowiedź opartą na danych organizacyjnych, 5) System śledzi historię konwersacji, co pozwala na prowadzenie spójnych rozmów. Ta architektura zapewnia, że odpowiedzi modelu są oparte na aktualnych danych organizacyjnych, respektują polityki bezpieczeństwa i są zgodne z wewnętrznymi wytycznymi.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Jak przygotować dane do fine-tuningu modelu LLM?\",\n",
    "        \"completion\": \"Aby przygotować dane do fine-tuningu modelu LLM, należy: 1) Zebrać pary pytanie-odpowiedź reprezentatywne dla docelowego zastosowania, 2) Upewnić się, że dane są wysokiej jakości i poprawne merytorycznie, 3) Sformatować dane zgodnie z wymaganiami platformy do fine-tuningu (np. w formacie JSONL), 4) Podzielić dane na zbiór treningowy i walidacyjny, 5) Upewnić się, że rozkład tematów w danych jest zrównoważony, 6) Dla każdego promptu przygotować kompletną i dokładną odpowiedź, 7) Usunąć poufne lub wrażliwe informacje, które nie powinny być w modelu. W naszej organizacji używamy wewnętrznej dokumentacji, wiedzy eksperckiej i wcześniejszych interakcji z klientami jako źródeł danych treningowych.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Jakie są podstawowe komendy Spark SQL używane w naszych projektach?\",\n",
    "        \"completion\": \"W naszych projektach często używamy następujących komend Spark SQL: 1) CREATE TABLE - do tworzenia nowych tabel, 2) SELECT - do wybierania danych z tabel, 3) FROM - określanie źródła danych, 4) WHERE - filtrowanie wierszy, 5) GROUP BY - grupowanie danych, 6) JOIN - łączenie tabel, 7) ORDER BY - sortowanie wyników, 8) WITH - definiowanie tymczasowych podzapytań, 9) MERGE INTO - operacje upsert, 10) EXPLAIN - analiza planu wykonania zapytania. Pamiętaj, że w naszej organizacji preferujemy Delta Lake jako format do przechowywania danych, więc często używamy też specyficznych komend Delta jak OPTIMIZE, VACUUM czy DESCRIBE HISTORY.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Jaka jest nasza procedura rejestrowania modeli ML w organizacji?\",\n",
    "        \"completion\": \"Nasza procedura rejestrowania modeli ML obejmuje następujące kroki: 1) Udokumentowanie modelu (algorytm, hiperparametry, metryki, zbiór danych), 2) Przeprowadzenie testów modelu pod kątem dokładności i wydajności, 3) Rejestracja modelu w Azure ML Registry lub MLflow Registry z odpowiednimi tagami i metadanymi, 4) Przypisanie wersji modelu, 5) Dodanie opisu modelu w wewnętrznym katalogu modeli, 6) Przesłanie modelu do przeglądu przez zespół ds. zarządzania modelami, 7) Po zatwierdzeniu, oznaczenie modelu jako gotowego do produkcji, 8) Skonfigurowanie monitoringu modelu w środowisku produkcyjnym. Każdy model musi przejść ten proces przed wdrożeniem produkcyjnym.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Jak skonfigurować połączenie do naszej bazy danych SQL z Databricks?\",\n",
    "        \"completion\": \"Aby skonfigurować połączenie do naszej firmowej bazy danych SQL z Databricks, wykonaj następujące kroki: 1) Otwórz zakładkę 'Secrets' w Databricks, 2) Dodaj poświadczenia do zakresu 'company-db-credentials' (w tym 'jdbc-url', 'username' i 'password'), 3) W notebooku użyj następującego kodu: ```python\\njdbc_url = dbutils.secrets.get('company-db-credentials', 'jdbc-url')\\nusername = dbutils.secrets.get('company-db-credentials', 'username')\\npassword = dbutils.secrets.get('company-db-credentials', 'password')\\n\\n# Połączenie z bazą danych\\ndf = spark.read\\n  .format('jdbc')\\n  .option('url', jdbc_url)\\n  .option('dbtable', 'schema.table_name')\\n  .option('user', username)\\n  .option('password', password)\\n  .load()\\n```\\nPamiętaj, żeby nie przechowywać poświadczeń bezpośrednio w kodzie i zawsze używać mechanizmu Secrets. Jeśli masz problemy z dostępem, skontaktuj się z zespołem DBA.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Jakie są nasze standardy dotyczące dokumentacji kodu?\",\n",
    "        \"completion\": \"Nasze standardy dokumentacji kodu obejmują: 1) Każda funkcja i klasa musi mieć docstring w formacie NumPy/PyDoc, 2) Docstring powinien opisywać cel, parametry, zwracane wartości i wyjątki, 3) Skomplikowane fragmenty kodu powinny mieć komentarze wyjaśniające logikę, 4) Każdy moduł powinien zawierać opis swojego przeznaczenia na początku pliku, 5) Zmienne i funkcje powinny mieć opisowe nazwy, 6) Ważne algorytmy powinny mieć odniesienia do dokumentów projektowych lub literatury, 7) Zmiany w kodzie muszą być udokumentowane w pliku CHANGELOG.md, 8) Kod powinien zawierać przykłady użycia dla głównych funkcji, 9) Dokumentacja powinna być aktualizowana przy każdej istotnej zmianie kodu. Szczegółowe wytyczne znajdziesz w wewnętrznym repozytorium 'coding-standards'.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Jak uzyskać dostęp do naszych wewnętrznych dashboardów analitycznych?\",\n",
    "        \"completion\": \"Aby uzyskać dostęp do naszych wewnętrznych dashboardów analitycznych: 1) Otwórz przeglądarkę i przejdź do https://analytics.nasza-firma.com, 2) Zaloguj się używając swoich firmowych poświadczeń (SSO), 3) Po zalogowaniu będziesz miał dostęp do różnych folderów z raportami w zależności od swoich uprawnień, 4) Główne dashboardy znajdują się w folderze 'Corporate Dashboards', 5) Dashboardy departamentowe znajdują się w folderach z nazwami odpowiednich działów, 6) Jeśli potrzebujesz dostępu do dodatkowych raportów, wypełnij formularz 'Dashboard Access Request' dostępny w intranecie i uzyskaj zgodę właściciela danego dashboardu. W razie problemów technicznych, skontaktuj się z zespołem BI pod adresem bi-support@nasza-firma.com.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Konwersja na DataFrame\n",
    "df_training = pd.DataFrame(training_data)\n",
    "print(f\"Przygotowano {len(df_training)} par pytanie-odpowiedź do fine-tuningu\")\n",
    "\n",
    "# Wyświetlenie kilku przykładów\n",
    "print(\"\\nPrzykłady:\\n\")\n",
    "for i in range(min(3, len(df_training))):\n",
    "    print(f\"Prompt: {df_training.iloc[i]['prompt']}\")\n",
    "    print(f\"Completion: {df_training.iloc[i]['completion'][:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Podział danych na zbiory treningowy i walidacyjny\n",
    "\n",
    "Aby kontrolować jakość fine-tuningu, podzielimy dane na zbiory treningowy i walidacyjny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podział danych na zbiory treningowy i walidacyjny\n",
    "train_df, val_df = train_test_split(df_training, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Zbiór treningowy: {len(train_df)} przykładów\")\n",
    "print(f\"Zbiór walidacyjny: {len(val_df)} przykładów\")\n",
    "\n",
    "# Zapisanie danych do plików JSONL (wymagany format dla fine-tuningu OpenAI)\n",
    "train_jsonl_path = \"train_data.jsonl\"\n",
    "val_jsonl_path = \"val_data.jsonl\"\n",
    "\n",
    "# Funkcja pomocnicza do zapisywania w formacie JSONL\n",
    "def df_to_jsonl(df, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        for _, row in df.iterrows():\n",
    "            json_str = json.dumps({\"prompt\": row['prompt'], \"completion\": row['completion']})\n",
    "            f.write(json_str + '\\n')\n",
    "\n",
    "# Zapisanie danych\n",
    "df_to_jsonl(train_df, train_jsonl_path)\n",
    "df_to_jsonl(val_df, val_jsonl_path)\n",
    "\n",
    "print(f\"Zapisano dane treningowe do {train_jsonl_path}\")\n",
    "print(f\"Zapisano dane walidacyjne do {val_jsonl_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Przygotowanie do fine-tuningu za pomocą MosaicML w Databricks\n",
    "\n",
    "MosaicML to narzędzie w Databricks, które ułatwia fine-tuning modeli LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przygotowanie do fine-tuningu za pomocą MosaicML w Databricks\n",
    "# Ta część kodu może wymagać dostosowania w zależności od dostępnej wersji MosaicML i Databricks\n",
    "\n",
    "# Import wymaganych bibliotek\n",
    "try:\n",
    "    from transformers import TrainingArguments, Trainer, AutoModelForCausalLM, AutoTokenizer\n",
    "    from datasets import load_dataset\n",
    "    import torch\n",
    "    print(\"Biblioteki do fine-tuningu zostały zaimportowane pomyślnie\")\n",
    "except ImportError as e:\n",
    "    print(f\"Błąd importu bibliotek: {str(e)}\")\n",
    "    print(\"Upewnij się, że zainstalowano wszystkie wymagane biblioteki\")\n",
    "\n",
    "# Sprawdzenie dostępności GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU jest dostępne: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Liczba dostępnych GPU: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    print(\"GPU nie jest dostępne. Fine-tuning może być bardzo powolny na CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Definicja parametrów fine-tuningu\n",
    "\n",
    "Określimy teraz parametry procesu fine-tuningu, takie jak model bazowy i hiperparametry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definicja parametrów fine-tuningu\n",
    "\n",
    "# Model bazowy - możesz użyć różnych modeli w zależności od dostępności\n",
    "# Na potrzeby warsztatów używamy mniejszego modelu, który nie wymaga dużych zasobów\n",
    "base_model_name = \"databricks/dolly-v2-3b\"  # Alternatywie: \"EleutherAI/gpt-neo-1.3B\" lub inny dostępny model\n",
    "\n",
    "# Parametry treningu\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine-tuned-model\",\n",
    "    num_train_epochs=3,               # Liczba epok treningu\n",
    "    per_device_train_batch_size=2,    # Rozmiar batcha (dostosuj do pamięci GPU)\n",
    "    per_device_eval_batch_size=2,     # Rozmiar batcha dla walidacji\n",
    "    warmup_steps=500,                 # Liczba kroków rozgrzewki\n",
    "    weight_decay=0.01,                # Regularyzacja L2\n",
    "    logging_dir=\"./logs\",             # Katalog logów\n",
    "    logging_steps=100,                # Co ile kroków logować\n",
    "    evaluation_strategy=\"steps\",      # Strategia ewaluacji\n",
    "    eval_steps=500,                   # Co ile kroków ewaluować\n",
    "    save_steps=1000,                  # Co ile kroków zapisywać model\n",
    "    fp16=True,                        # Użycie precyzji połówkowej (FP16) dla przyspieszenia\n",
    "    gradient_accumulation_steps=4,    # Akumulacja gradientów\n",
    "    save_total_limit=2,               # Limit zapisanych modeli\n",
    "    load_best_model_at_end=True,      # Załadowanie najlepszego modelu na końcu\n",
    "    metric_for_best_model=\"eval_loss\",# Metryka do wyboru najlepszego modelu\n",
    "    greater_is_better=False           # Czy wyższa wartość metryki jest lepsza\n",
    ")\n",
    "\n",
    "print(\"Parametry fine-tuningu zostały zdefiniowane\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Przygotowanie modelu i tokenizera\n",
    "\n",
    "Załadujemy model bazowy i tokenizer, które będą dostrajane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przygotowanie modelu i tokenizera\n",
    "try:\n",
    "    # Załadowanie modelu i tokenizera\n",
    "    print(f\"Ładowanie modelu {base_model_name}...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    \n",
    "    # Upewnienie się, że tokenizer ma ustawiony token końca sekwencji\n",
    "    if tokenizer.eos_token is None:\n",
    "        tokenizer.eos_token = tokenizer.pad_token or \"</s>\"\n",
    "    \n",
    "    print(\"Model i tokenizer zostały załadowane pomyślnie\")\n",
    "    print(f\"Liczba parametrów modelu: {model.num_parameters():,}\")\n",
    "except Exception as e:\n",
    "    print(f\"Błąd podczas ładowania modelu: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Przygotowanie danych dla Transformers\n",
    "\n",
    "Przygotujemy nasze dane w formacie odpowiednim dla biblioteki Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przygotowanie danych dla Transformers\n",
    "try:\n",
    "    # Załadowanie danych z plików JSONL\n",
    "    train_dataset = load_dataset('json', data_files=train_jsonl_path, split='train')\n",
    "    val_dataset = load_dataset('json', data_files=val_jsonl_path, split='train')\n",
    "    \n",
    "    print(f\"Załadowano {len(train_dataset)} przykładów treningowych i {len(val_dataset)} przykładów walidacyjnych\")\n",
    "    \n",
    "    # Funkcja do tokenizacji danych\n",
    "    def tokenize_function(examples):\n",
    "        # Formatowanie promptów i odpowiedzi\n",
    "        prompts = [f\"Pytanie: {p}\\nOdpowiedź:\" for p in examples[\"prompt\"]]\n",
    "        completions = [f\" {c}{tokenizer.eos_token}\" for c in examples[\"completion\"]]\n",
    "        \n",
    "        # Tokenizacja promptów\n",
    "        tokenized_prompts = tokenizer(prompts, truncation=True, max_length=512)\n",
    "        \n",
    "        # Tokenizacja completions\n",
    "        tokenized_completions = tokenizer(completions, truncation=True, max_length=512)\n",
    "        \n",
    "        # Przygotowanie pełnych tokenizowanych przykładów (prompt + completion)\n",
    "        result = {\n",
    "            \"input_ids\": [],\n",
    "            \"attention_mask\": [],\n",
    "            \"labels\": []\n",
    "        }\n",
    "        \n",
    "        for prompt_ids, prompt_mask, compl_ids in zip(\n",
    "            tokenized_prompts[\"input_ids\"], \n",
    "            tokenized_prompts[\"attention_mask\"],\n",
    "            tokenized_completions[\"input_ids\"]\n",
    "        ):\n",
    "            # Połączenie prompt i completion\n",
    "            input_ids = prompt_ids + compl_ids[1:]  # Pomijamy pierwszy token z completion (zwykle jest to space)\n",
    "            attention_mask = prompt_mask + [1] * (len(compl_ids) - 1)\n",
    "            \n",
    "            # Dla etykiet: -100 dla promptu (nie liczymy loss) i rzeczywiste ID tokenów dla completion\n",
    "            labels = [-100] * len(prompt_ids) + compl_ids[1:]\n",
    "            \n",
    "            # Przycinanie do maksymalnej długości\n",
    "            max_length = 1024\n",
    "            if len(input_ids) > max_length:\n",
    "                input_ids = input_ids[:max_length]\n",
    "                attention_mask = attention_mask[:max_length]\n",
    "                labels = labels[:max_length]\n",
    "            \n",
    "            result[\"input_ids\"].append(input_ids)\n",
    "            result[\"attention_mask\"].append(attention_mask)\n",
    "            result[\"labels\"].append(labels)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    # Tokenizacja danych\n",
    "    tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "    tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "    \n",
    "    print(\"Dane zostały przygotowane do fine-tuningu\")\n",
    "except Exception as e:\n",
    "    print(f\"Błąd podczas przygotowywania danych: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Przeprowadzenie fine-tuningu\n",
    "\n",
    "Teraz przeprowadzimy właściwy proces fine-tuningu modelu.\n",
    "\n",
    "**Uwaga**: Ten proces może być czasochłonny i wymagać znacznych zasobów GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przeprowadzenie fine-tuningu\n",
    "# UWAGA: Ten proces może być czasochłonny i wymagać znacznych zasobów GPU\n",
    "# Na potrzeby warsztatów można ograniczyć liczbę epok lub użyć mniejszego modelu\n",
    "\n",
    "try:\n",
    "    # Inicjalizacja trenera\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_val_dataset\n",
    "    )\n",
    "    \n",
    "    print(\"Rozpoczynam fine-tuning...\")\n",
    "    # Uruchomienie treningu\n",
    "    trainer.train()\n",
    "    \n",
    "    print(\"Fine-tuning został zakończony\")\n",
    "    \n",
    "    # Zapisanie modelu\n",
    "    fine_tuned_model_path = \"./fine-tuned-model/final\"\n",
    "    trainer.save_model(fine_tuned_model_path)\n",
    "    tokenizer.save_pretrained(fine_tuned_model_path)\n",
    "    \n",
    "    print(f\"Model został zapisany w {fine_tuned_model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Błąd podczas fine-tuningu: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Rejestracja modelu w Azure AI Foundry (Azure ML Model Registry)\n",
    "\n",
    "Teraz zarejestrujemy dostrojony model w Azure ML Model Registry, aby można go było łatwo używać w różnych aplikacjach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rejestracja modelu w Azure ML Model Registry\n",
    "try:\n",
    "    # Ścieżka do zapisanego modelu\n",
    "    model_path = fine_tuned_model_path\n",
    "    \n",
    "    # Ustawienie tagu dla modelu\n",
    "    tags = {\n",
    "        \"base_model\": base_model_name,\n",
    "        \"task\": \"question-answering\",\n",
    "        \"domain\": \"enterprise-databot\",\n",
    "        \"training_data_size\": str(len(train_dataset)),\n",
    "        \"created_by\": \"workshop-participant\"\n",
    "    }\n",
    "    \n",
    "    # Rejestracja modelu\n",
    "    model_name = \"enterprise-databot-finetuned\"\n",
    "    model_description = \"Fine-tuned model for enterprise DataBot based on \" + base_model_name\n",
    "    \n",
    "    # Zarejestruj model w Azure ML\n",
    "    registered_model = Model.register(\n",
    "        workspace=ws,\n",
    "        model_path=model_path,\n",
    "        model_name=model_name,\n",
    "        description=model_description,\n",
    "        tags=tags\n",
    "    )\n",
    "    \n",
    "    print(f\"Model został zarejestrowany: {registered_model.name}, wersja: {registered_model.version}\")\n",
    "    print(f\"ID modelu: {registered_model.id}\")\n",
    "except Exception as e:\n",
    "    print(f\"Błąd podczas rejestracji modelu: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Testowanie dostrojonego modelu\n",
    "\n",
    "Przetestujemy nasz dostrojony model i porównamy jego odpowiedzi z modelem bazowym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testowanie dostrojonego modelu\n",
    "try:\n",
    "    # Załadowanie dostrojonego modelu\n",
    "    fine_tuned_model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_path)\n",
    "    fine_tuned_tokenizer = AutoTokenizer.from_pretrained(fine_tuned_model_path)\n",
    "    \n",
    "    # Załadowanie modelu bazowego (dla porównania)\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "    base_tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    \n",
    "    # Przykładowe pytania do testowania\n",
    "    test_questions = [\n",
    "        \"Co to jest Microsoft Fabric?\",\n",
    "        \"Jak działa architektura RAG w kontekście naszej organizacji?\",\n",
    "        \"Jakie są nasze standardy dokumentacji kodu?\"\n",
    "    ]\n",
    "    \n",
    "    # Funkcja do generowania odpowiedzi\n",
    "    def generate_response(model, tokenizer, question):\n",
    "        prompt = f\"Pytanie: {question}\\nOdpowiedź:\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        \n",
    "        # Przeniesienie na GPU, jeśli jest dostępne\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "            model = model.to(\"cuda\")\n",
    "        \n",
    "        # Generowanie odpowiedzi\n",
    "        outputs = model.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            max_length=1024, \n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        # Dekodowanie odpowiedzi\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Wyodrębnienie tylko części \"Odpowiedź:\"\n",
    "        answer_start = response.find(\"Odpowiedź:\")\n",
    "        if answer_start != -1:\n",
    "            response = response[answer_start + len(\"Odpowiedź:\"):].strip()\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    # Porównanie odpowiedzi\n",
    "    print(\"\\nPorównanie modelu bazowego i dostrojonego:\\n\")\n",
    "    for question in test_questions:\n",
    "        print(f\"Pytanie: {question}\")\n",
    "        \n",
    "        # Odpowiedź z modelu bazowego\n",
    "        base_response = generate_response(base_model, base_tokenizer, question)\n",
    "        print(f\"\\nOdpowiedź modelu bazowego:\\n{base_response[:500]}...\" if len(base_response) > 500 else f\"\\nOdpowiedź modelu bazowego:\\n{base_response}\")\n",
    "        \n",
    "        # Odpowiedź z dostrojonego modelu\n",
    "        fine_tuned_response = generate_response(fine_tuned_model, fine_tuned_tokenizer, question)\n",
    "        print(f\"\\nOdpowiedź dostrojonego modelu:\\n{fine_tuned_response[:500]}...\" if len(fine_tuned_response) > 500 else f\"\\nOdpowiedź dostrojonego modelu:\\n{fine_tuned_response}\")\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"Błąd podczas testowania modelu: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Integracja dostrojonego modelu z DataBotem\n",
    "\n",
    "Teraz zintegrujemy nasz dostrojony model z DataBotem stworzonym w poprzednim ćwiczeniu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integracja dostrojonego modelu z DataBotem\n",
    "# UWAGA: Ten kod zakłada, że wcześniej zdefiniowano klasę DataBot z poprzedniego ćwiczenia\n",
    "# Może wymagać dostosowania w zależności od implementacji DataBota\n",
    "\n",
    "try:\n",
    "    # Import potrzebnych klas z poprzedniego ćwiczenia\n",
    "    # W rzeczywistym scenariuszu te klasy byłyby zaimportowane z odpowiednich modułów\n",
    "    # Tutaj dla uproszczenia definiujemy podstawowy szkielet DataBota\n",
    "    \n",
    "    class SimpleDataBot:\n",
    "        def __init__(self, model_path=None, use_fine_tuned=True):\n",
    "            # Inicjalizacja tokenizera i modelu\n",
    "            if use_fine_tuned and model_path:\n",
    "                print(f\"Ładowanie dostrojonego modelu z {model_path}...\")\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "            else:\n",
    "                print(f\"Ładowanie modelu bazowego {base_model_name}...\")\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "            \n",
    "            # Przeniesienie modelu na GPU, jeśli dostępne\n",
    "            if torch.cuda.is_available():\n",
    "                self.model = self.model.to(\"cuda\")\n",
    "                \n",
    "            print(\"DataBot został zainicjalizowany\")\n",
    "        \n",
    "        def process_query(self, query):\n",
    "            # Przygotowanie promptu\n",
    "            prompt = f\"Pytanie: {query}\\nOdpowiedź:\"\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "            \n",
    "            # Przeniesienie na GPU, jeśli jest dostępne\n",
    "            if torch.cuda.is_available():\n",
    "                inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "            \n",
    "            # Generowanie odpowiedzi\n",
    "            outputs = self.model.generate(\n",
    "                inputs[\"input_ids\"], \n",
    "                max_length=1024, \n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "            \n",
    "            # Dekodowanie odpowiedzi\n",
    "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Wyodrębnienie tylko części \"Odpowiedź:\"\n",
    "            answer_start = response.find(\"Odpowiedź:\")\n",
    "            if answer_start != -1:\n",
    "                response = response[answer_start + len(\"Odpowiedź:\"):].strip()\n",
    "            \n",
    "            return response\n",
    "    \n",
    "    # Inicjalizacja DataBota z dostrojonym modelem\n",
    "    databot_fine_tuned = SimpleDataBot(model_path=fine_tuned_model_path, use_fine_tuned=True)\n",
    "    \n",
    "    # Testowanie DataBota\n",
    "    test_question = \"Jak skonfigurować połączenie do naszej bazy danych SQL z Databricks?\"\n",
    "    \n",
    "    print(f\"\\nPytanie: {test_question}\")\n",
    "    response = databot_fine_tuned.process_query(test_question)\n",
    "    print(f\"\\nOdpowiedź DataBota:\\n{response}\")\n",
    "except Exception as e:\n",
    "    print(f\"Błąd podczas integracji z DataBotem: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Monitorowanie i wersjonowanie modelu\n",
    "\n",
    "Sprawdźmy informacje o zarejestrowanym modelu i zobaczmy, jak można zarządzać jego wersjami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitorowanie i wersjonowanie modelu w Azure ML\n",
    "try:\n",
    "    # Pobranie zarejestrowanego modelu\n",
    "    model_versions = Model.list(ws, name=model_name)\n",
    "    \n",
    "    print(f\"\\nZarejestrowane wersje modelu '{model_name}':\")\n",
    "    for model in model_versions:\n",
    "        print(f\"  - Wersja: {model.version}, Utworzono: {model.creation_time}, ID: {model.id}\")\n",
    "    \n",
    "    # Dodanie nowej wersji modelu (symulacja)\n",
    "    print(\"\\nDodawanie nowych tagów do modelu...\")\n",
    "    latest_model = model_versions[0]  # Najnowsza wersja\n",
    "    \n",
    "    # Dodanie nowych tagów\n",
    "    latest_model.add_tags({\"evaluation_accuracy\": \"0.85\", \"status\": \"production-ready\"})\n",
    "    \n",
    "    # Pobranie zaktualizowanych tagów\n",
    "    updated_model = Model(ws, name=model_name, version=latest_model.version)\n",
    "    \n",
    "    print(\"\\nZaktualizowane tagi modelu:\")\n",
    "    for tag_name, tag_value in updated_model.tags.items():\n",
    "        print(f\"  - {tag_name}: {tag_value}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Błąd podczas demonstracji wersjonowania modelu: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Zadania do wykonania\n",
    "\n",
    "1. Dodaj więcej przykładów do zbioru treningowego, aby poprawić jakość dostrojonego modelu\n",
    "2. Zmodyfikuj parametry fine-tuningu (np. liczbę epok, rozmiar batcha) i zobacz, jak wpływa to na wyniki\n",
    "3. Porównaj odpowiedzi modelu bazowego i dostrojonego na pytania specyficzne dla twojej organizacji\n",
    "4. Zaimplementuj prostą metrykę oceny jakości odpowiedzi generowanych przez model\n",
    "\n",
    "## Zadania dodatkowe\n",
    "\n",
    "Jeśli masz więcej czasu, możesz spróbować:\n",
    "\n",
    "1. Eksperymentowanie z różnymi parametrami fine-tuningu (liczba epok, rozmiar batcha, tempo uczenia)\n",
    "2. Dodanie większej liczby przykładów do zbioru treningowego lub przygotowanie własnych przykładów\n",
    "3. Implementacja mechanizmu ewaluacji jakości modelu z wykorzystaniem metryk takich jak BLEU, ROUGE itp.\n",
    "4. Przygotowanie pipeline'u CI/CD do automatycznego fine-tuningu i wdrażania modelu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
